---
title: "W271 Lab 3 Spring 2016"
author: "Megan Jasek, Rohan Thakur, Charles Kekeh"
date: "Sunday, April 24, 2016"
output: pdf_document
---

```{r,include=FALSE}
library(pastecs)
library(knitr)
library(psych)
library(astsa)
library(zoo)    
library(forecast)
library(quantmod)
library(fGarch)
library(tseries)
library(ggplot2)
library(stargazer)
library(car)
library(ggplot2)
library(sandwich)
library(lmtest)
library(pastecs)
library(ivpack)
library(stats)
opts_chunk$set(tidy.opts=list(width.cutoff=60))

# Set the seed so that reported numbers remain stable
set.seed(1)
```

Below, we define some functions we will be using in the problem set:
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Functions for Parts 2, 3, 4
get.best.arima <- function(x.ts, maxord = c(1,1,1))
{
  best.aic <- 1e8
  all.aics <- vector()
  all.models <- vector()
  n <- length(x.ts)
  for (p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3]) 
  {
    fit <- arima(x.ts, order = c(p,d,q), method="ML")
    fit.aic <- -2 * fit$loglik + (log(n) + 1) * length(fit$coef)
    if (fit.aic < best.aic) 
    {
      best.aic <- fit.aic
      best.fit <- fit
      best.model <- c(p,d,q) 
    }
    all.aics <- c(all.aics, fit.aic)
    all.models <- c(all.models, sprintf("(%d, %d, %d)", p,d,q))
  }
  list(best=list(best.aic, best.fit, best.model), others=data.frame(aics=all.aics, models=all.models))
}

get.best.sarima <- function(x.ts, maxord=c(1,1,1,1,1,1), freq=frequency(x.ts))
{
  best.aic <- 1e8
  all.aics <- vector()
  all.models <- vector()
  n <- length(x.ts)
  for (p in 0:maxord[1]) for (d in 0:maxord[2]) for (q in 0:maxord[3])
    for (P in 0:maxord[3]) for (D in 0:maxord[4]) for (Q in 0:maxord[5])
    {
      fit <- arima(x.ts, order = c(p,d,q),
                   seasonal=list(order= c(P,D,Q),freq),
                   method ="CSS", optim.control = list(maxit = 10000))
      fit.aic <- -2*fit$loglik + (log(n) + 1) * length(fit$coef)
      if (fit.aic < best.aic){
        best.aic <- fit.aic
        best.fit <- fit
        best.model <- c(p,d,q,P,D,Q)
      }
      all.aics <- c(all.aics, fit.aic)
      all.models <- c(all.models, sprintf("(%d, %d, %d, %d, %d, %d)", p,d,q,P,D,Q))
    }
  list(best=list(best.aic, best.fit, best.model), others=data.frame(aics=all.aics, models=all.models))
}

plot.time.series <- function(x.ts,bins=30, name)
{
  str(x.ts)
  par(mfrow=c(2,2))
  hist(x.ts,bins, main = paste("Histogram of", name, sep=" "),
       xlab="Values")
  plot(x.ts, main=paste("Plot of", name, sep=" "),
       ylab="Values", xlab="Time Period")
  acf(x.ts, main = paste("ACF of", name, sep=" "))
  pacf(x.ts, main = paste("PACF of", name, sep=" "))
}

plot.residuals.ts <- function(x.mod, model_name)
{
  par(mfrow=c(1,1))
  hist(x.mod$residuals,30, main=paste("Histogram of", model_name,
                                      "Residuals", sep=" "), xlab="Values")
  par(mfrow=c(2,2))
  plot(x.mod$residuals, fitted(x.mod), main=paste(model_name, "Fitted vs. Residuals", sep=" "),
       ylab="Fitted Values", xlab="Residuals")
  plot(x.mod$residuals, main=paste(model_name, "Residuals", sep=" "),
       ylab=paste("Residuals", sep=" "))
  acf(x.mod$residuals, main = paste("ACF of", model_name, sep=" "))
  pacf(x.mod$residuals, main = paste("PACF of", model_name, sep=" "))
  Box.test(x.mod$residuals, type="Ljung-Box")
}

estimate.ar <- function(x.ts)
{
  x.ar = ar(x.ts)
  print("Difference in AICs")
  print(x.ar$aic)
  print("AR parameters")
  print(x.ar$ar)
  print("AR order")
  print(x.ar$order)
  return(x.ar)
}

plot.orig.model.resid <- function(x.ts, x.mod, model_name, xlim, ylim)
{
  df <- data.frame(cbind(x.ts,fitted(x.mod),x.mod$residuals ))
  class(df)  
  stargazer(df, type="text", title="Descriptive Stat", digits=1)

  summary(x.ts)
  summary(x.mod$residuals)
  par(mfrow=c(1,1))
  plot.ts(x.ts, col="red", 
        main=paste("Orivinal vs Estimated", model_name,
                   "Series with Resdiauls", sep=" "),
        ylab="Original and Estimated Values",
        xlim=xlim, ylim=ylim, pch=1, lty=2)
  par(new=T)
  plot.ts(fitted(x.mod),col="blue",axes=T,xlab="",ylab="",
        xlim=xlim, ylim=ylim, lty=1) 
  leg.txt <- c("Original Series", "Estimated Series", "Residuals")
  legend("topleft", legend=leg.txt, lty=c(2,1,2), 
       col=c("red","blue","green"), bty='n', cex=1)
  par(new=T)
  plot.ts(x.mod$residuals,axes=F,xlab="",ylab="",col="green",
        xlim=xlim, ylim=ylim, lty=2, pch=1, col.axis="green")
  axis(side=4, col="green")
  mtext("Residuals", side=4, line=2,col="green")
}

plot.model.forecast <- function(x.mod, mod.fcast, num_steps, x, y)
{
  par(mfrow=c(1,1))
  plot(mod.fcast,
       main=paste(num_steps, "-Step Ahead Forecast and Original & Estimated Series", sep=""),
       xlab="Simulated Time Period",
       ylab="Original, Estimated, and Forecasted Values",
       xlim=x, ylim=y, lty=2, lwd=1.5)
  par(new=T)
  plot.ts(fitted(x.mod),col="blue", 
        lty=2, lwd=2, xlab="", ylab="", xlim=x, ylim=y)
  leg.txt <- c("Original Series", "Estimated Series", "Forecast")
  legend("topleft", legend=leg.txt, lty=c(2,2,1), lwd=c(1,2,2),
       col=c("black","blue","blue"), bty='n', cex=1)
}

```

\newpage

Part 1 (25 points): Modeling House Values
=========================================

Step 1 - Univariate Analysis
----------------------------
1. **Crime Rate** - This variable is positively skewed, with 90% of datapoints having a crime rate below 11.2%, but outliers above that going upto 89%. We take the log to create a new variable before proceeding.
2. **nonRetailBusiness** - Has a suspiciously high mode at 0.18, which may indicate that a lot of the data points come from the same neighbourhood, which would explain the high number of occurences of a single value.
3. **withWater** - This is a categorical variable. 6.75% of homes in the given sample are in neighbourhoods within 5 miles of a water body.
4. **ageHouse** - This value is in percentage terms and not in strict proportion like other variables in the dataset. Over 50% of the houses in the dataset are in neighbourhoods with a proportion of houses older than 1950 that is greater than 78%
5. **distanceToCity** - 75% of the houses are less than 15 miles away from a city, and 90% are less than 25 miles away. However, the variable has a lage outlier, which is almost 55 miles away from a city. We take a log of the variable before proceeding, in order to make it more evenly distributed.
6. **distanceToHighway** - Definition of the variable is not provided in the dictionary. We see that there are 104 datapoints, exactly 24 miles away from the highway, so we assume this variable measures distance of a neighbourhood from the highway. This is exactly the same number of points for which nonRetailBusiness has a value of 0.18, so it further strengthens the argument that a lot of the datapoints seem to be for houses in the same or very close neighbourhood.
7. **pupilTeacherRatio** - We find another variable with a high modal value of exactly 23.2 pupils per teacher. Furthers the above argument that a large part of the sample is taken from a single neighbourhood.
8. **pctLowIncome** - 90% of the homes come from neighbourhoods with less than 30% households being low-income, however we do have values going up to 49% in the dataset.
9. **homeValue** - The distribution has 95% of houses valued at well below $1 million, however, there are outliers above that value upto $1.125 million. We take the log of the variable to make it closer to a normal distribution.
10. **pollutionIndex** - Has a scattered distribution with a median of 38.8, and a large outlier at 72.1.
11. **nBedRooms** - Close to normal distribution, with the mean and median around 4.25 bedrooms on average for a single family home, however there are small, as well as large outliers in the distribution. 

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
q1.dataset = read.csv('houseValueData.csv')

str(q1.dataset)
summary(q1.dataset)

#Performing univariate analysis
#Crime Rate
summary(q1.dataset$crimeRate_pc)
quantile(q1.dataset$crimeRate_pc, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$crimeRate_pc, breaks=60, col="blue", main = "Distribution of Crime Rate", xlab = "Crime Rate")
#This variable is extremely +vely skewed, so we take the log
q1.dataset$logCrimeRate_pc = log(q1.dataset$crimeRate_pc)
hist(q1.dataset$logCrimeRate_pc, breaks=60, col="blue", main = "Distribution of log(Crime Rate)", xlab = "log(Crime Rate)")

#nonRetailBusiness
summary(q1.dataset$nonRetailBusiness)
quantile(q1.dataset$nonRetailBusiness, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$nonRetailBusiness, breaks=60, col="blue", main = "Distribution of nonRetailBusiness", xlab = "nonRetailBusiness")
head(q1.dataset[order(q1.dataset$nonRetailBusiness, decreasing = TRUE), 
                c("nonRetailBusiness")], n=50)
tail(sort(table(q1.dataset$nonRetailBusiness)),5)
#suspicious that this has such a large modal value. May be some coded val

#withWater
summary(q1.dataset$withWater)
#7% have water

#ageHouse
summary(q1.dataset$ageHouse)
quantile(q1.dataset$ageHouse, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$ageHouse, breaks=60, col="blue", main = "Distribution of ageHouse", xlab = "ageHouse")
#Looks like a % value. 
#May require a power transformation

#disttocity
summary(q1.dataset$distanceToCity)
quantile(q1.dataset$distanceToCity, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$distanceToCity, breaks=60, col="blue", main = "Distribution of distanceToCity", xlab = "DistanceToCity")
q1.dataset$logDistanceToCity = log(q1.dataset$distanceToCity)
#skewed with a large outlier at the end. Keep in mind while running model

#pupilTeacher
summary(q1.dataset$pupilTeacherRatio)
quantile(q1.dataset$pupilTeacherRatio, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$pupilTeacherRatio, breaks=60, col="blue", main = "Distribution of pupilTeacher Ratio", xlab = "pupilTeacher Ratio")
tail(sort(table(q1.dataset$pupilTeacherRatio)),5)
#High mode at 23.2, suspicious 

#dist to highway
summary(q1.dataset$distanceToHighway)
quantile(q1.dataset$distanceToHighway, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$distanceToHighway, breaks=60, col="blue",  main = "Distribution of DistanceToHighway",  xlab = "DistanceToHighway")
tail(sort(table(q1.dataset$distanceToHighway)),5)
#Very strange that so many values are exactly 24. May not be best thing for regression.

#pctlowincome
summary(q1.dataset$pctLowIncome)
quantile(q1.dataset$pctLowIncome, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$pctLowIncome, breaks=60, col="blue", main = "Distribution of % low income", xlab = "% Low Income")
tail(sort(table(q1.dataset$pctLowIncome)),5)
#slight neg skew

#Home value
summary(q1.dataset$homeValue)
quantile(q1.dataset$homeValue, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$homeValue, breaks=60, col="blue", main = "Distribution of Home Value", xlab = "Home Value")
q1.dataset$logHomeValue = log(q1.dataset$homeValue)
hist(q1.dataset$logHomeValue, breaks=60, col="blue", main = "Distribution of Log(Home Value)",  xlab = "Log(Home Value)")
#Pretty normal

#poll Index
summary(q1.dataset$pollutionIndex)
quantile(q1.dataset$pollutionIndex, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$pollutionIndex, breaks=60, col="blue",  main = "Distribution of Pollution Index",   xlab = "Pollution Index")
#scattered dist, one high outlier at 72

#nbedrooms
summary(q1.dataset$nBedRooms)
quantile(q1.dataset$nBedRooms, 
         probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1))
hist(q1.dataset$nBedRooms, breaks=60, col="blue", main = "Distribution of nBedRooms", xlab = "nBedRooms")
#Pretty normal
```



\newpage

Step 2 - Bivariate Analysis
----------------------------
We examine bivariate correlations and scatterplots for all (transformed) variables in the dataset.

##Conclusions

1. A lot of variables show strong correlations with each other in the dataset (absolute val of correlation > 0.7). We must be wary of multicollinearity when including these variables together in our regression models which would make our model coefficients lose precision. At the same time, it is important to include necessary variables in order to prevent any omitted variable bias.
 * logCrimeRate shows a strong correlation with logdistanceToCity, distanceToHighway and pollutionIndex
 * nonRetailBusiness shows a strong correlation with logdistanceToCity and pollutionIndex
 * ageHouse also shows a strong correlation with logdistanceToCity and pollutionIndex
 * logDistancetoCity shows a strong correlation with pollutionIndex
 


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
panel.hist <- function(x, ...)
   {
     usr <- par("usr"); on.exit(par(usr))
     par(usr = c(usr[1:2], 0, 1.5) )
     h <- hist(x, plot = FALSE)
     breaks <- h$breaks; nB <- length(breaks)
     y <- h$counts; y <- y/max(y)
     rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
   {
     usr <- par("usr"); on.exit(par(usr))
     par(usr = c(0, 1, 0, 1))
     r <- abs(cor(x, y))
     txt <- format(c(r, 0.123456789), digits = digits)[1]
     txt <- paste0(prefix, txt)
     if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
     text(0.5, 0.5, txt, cex = cex.cor * r)
}

pairs(logHomeValue~logCrimeRate_pc+nonRetailBusiness+withWater+ageHouse+
        logDistanceToCity+distanceToHighway+pupilTeacherRatio+pctLowIncome+
        pollutionIndex+nBedRooms, data=q1.dataset, upper.panel=panel.smooth, 
      lower.panel=panel.cor, diag.panel=panel.hist)
```


\newpage

Step 3 - Model Estimation
-------------------------
We start off with a naive approach, including all variables in the regression to observe results.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
model.1 = lm(logHomeValue~logCrimeRate_pc+nonRetailBusiness+withWater+ageHouse+
        logDistanceToCity+distanceToHighway+pupilTeacherRatio+pctLowIncome+
        pollutionIndex+nBedRooms, data=q1.dataset)
summary(model.1)
AIC(model.1)
BIC(model.1)
```

In this model, we see that several variables do not have statistical significance. We see that the coefficients lack precision, having extremely high standard errors. 

Before we move on to more parsimonious models, we will examine interaction variables to see if they add any explanatory power to our model. We have two categorical variables in our dataset: withWater and distanceToHighway (though a numerical variable, it has only nine distinct values, effectively functioning as a categorical). We add all possible interactions with these variables to see if we obtain any noteworthy result.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
model.2 = lm(logHomeValue~logCrimeRate_pc+nonRetailBusiness+withWater+ageHouse+
        logDistanceToCity+distanceToHighway+pupilTeacherRatio+pctLowIncome+
        pollutionIndex+nBedRooms+distanceToHighway:logCrimeRate_pc+distanceToHighway:nonRetailBusiness+distanceToHighway:ageHouse+distanceToHighway:pupilTeacherRatio+distanceToHighway:pctLowIncome+distanceToHighway:pollutionIndex+distanceToHighway:nBedRooms+
        withWater:pollutionIndex+withWater:logCrimeRate_pc+withWater:nonRetailBusiness+withWater:ageHouse+withWater:logDistanceToCity+withWater:pupilTeacherRatio+withWater:pctLowIncome+withWater:nBedRooms, data=q1.dataset)
summary(model.2)
AIC(model.2)
BIC(model.2)
waldtest(model.1, model.2)
```

We do see some additional explanatory power through the addition of the interaction terms as we obtain a model with higher R square, lower AIC and lower BIC, as well as a significant Wald Test p value. However, apart from having a model which is extremely difficult to interpret, we also notice that most of the coefficient estimates are extremely small, having very little practical significance.

Now, we want to narrow down our model to include only variables that really add to the explanatory power of the model, that reduce multicollinearity, that provide some valuable practical significance, while meeting the think-tank's specific ask of desirable neighbourhood features and environmental features' relation to home values.

We remove the following variables:

1. nonRetailBusiness: It has a high correlation with several of the variables in the dataset, and is not of direct interest to answering the question asked.
2. ageHouse: It is not of direct consequence to the question asked.
3. DistanceToCity has a high correlation with pollutionIndex, a variable we are definitely interested in, so we remove it to reduce the loss of precision that comes with multicollinearity
4. nBedRooms: It is not of direct consequence to the question asked.
5. distanceToHighway interactions except the interaction with log crime: This is the only interaction with a variable still in the model which has statistical and practical significance.
6. withWater interactions except the interaction with pollutionIndex: This interaction seems to have some practical as well as statistical significance.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
model.3 = lm(logHomeValue~logCrimeRate_pc+withWater+
        distanceToHighway+pctLowIncome+
        pollutionIndex+distanceToHighway:logCrimeRate_pc+
        withWater:pollutionIndex, data=q1.dataset)
summary(model.3)
AIC(model.3)
BIC(model.3)
```

We see that the distanceToHighway and pollutionIndex variables don't seem to have statistical significance. We will remove the distanceToHighway variable since it does not have direct consequence to the question asked. Further, we remove the logCrimeRate variable, since it has a high correlation with pollutionIndex, a variable of importance to us. We do this to observe if its removal increases the precision of the pollutonIndex variable. 
Note that this also means that we remove the interaction of distanceToHighway and logCrimeRate

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
model.4 = lm(logHomeValue~withWater+  pctLowIncome+
        pollutionIndex+withWater:pollutionIndex, data=q1.dataset)
summary(model.4)
AIC(model.4)
BIC(model.4)
```

We still obtain no significance for our pollutionIndex variable, and are unable to make any confident claims about this variable's impact on homeValue. Perhaps this variable, which was significant in earlier models with more variables, is losing precision due to an omitted variable bias. 

We try adding back the pupilTeacherRatio variable to see if that makes any difference to the model.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
model.5 = lm(logHomeValue~withWater+  pctLowIncome+pupilTeacherRatio+
        pollutionIndex+withWater:pollutionIndex, data=q1.dataset)
summary(model.5)
AIC(model.5)
BIC(model.5)
```

We obtain statistical significance for all our coefficients, while maintaining a high R square value. While AIC and BIC values are not the lowest, we prefer the parsimony of this model, and choose this as our final model to present to the think-tank.

We do not consider introducing instrument variables to the model for the following reasons:

1. None of our predictors seems to have any correlation with the error term, making them all exogenous (shown above).
2. While some variables do meet the criteria for instrument relevance, they are correlated with multiple variables in the model, so they do not make good overall candidates for instruments as they would introduct multicollinearity to the prediction of the variable for which they would serve as instruments.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
cor(q1.dataset$pctLowIncome, model.5$residuals)
cor(q1.dataset$pollutionIndex, model.5$residuals)
cor(q1.dataset$pupilTeacherRatio, model.5$residuals)

plot(q1.dataset$pctLowIncome, model.5$residuals)
plot(q1.dataset$pollutionIndex, model.5$residuals)
plot(q1.dataset$pupilTeacherRatio, model.5$residuals)
```

Now, we look at diagnostics for our final model.

1. The residuals vs fitted plot shows a very slight upward trend. The slope is negligible, so we assume zero conditional mean to hold.
2. Errors follow a close to normal distribution. In either case, we have 400 observations, enabling us to rely on OLS asymptotics
3. The scale-location plot shows some trend which is not a significant cause for concern. It shows some heteroskedasticity, which we account for by taking robust standard errors below. Variables in our model remain significant. The Wald-Test shows that the model also remains significant.
4. The Residuals vs Leverrage plot shows some outliers but no major cause for concern.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
coeftest(model.5)
waldtest(model.5, vcov=vcovHC)
plot(model.5)
```


\newpage

Step 4 - Final Model Conclusions
--------------------------------
##Conclusion
Below we present the a comparison of initial and the final models run. We see that despite removing 6 variables, we see a reduction of only around 4% in R square, implying that we have preserved most of the explanatory power of the model, while sticking with parsimony.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
stargazer(model.1, model.5, type="text")
```

##Explanation of the Model
The final results from the model we present to the think-tank are as follows:

1. Home values are 50% greater for homes located within 5 miles of water.
2. For every unit percentage increase in low-income households, home values are close to 3% lower. 
3. For every one unit increase in the pupilTeacherRatio in a neighbourhood, home values are close to 4% lower.
4. For an additional unit on the pollutionIndex, we expect to see a decrease in home value of 0.2% if it is in a neighbourhood not within 5 miles of a water body. However, if it is within 5 miles of a water body, we expect an almost 1% decrease in home value.


\newpage

Part 2 (25 points): Modeling and Forecasting a Real-World Macroeconomic / Financial time series
===============================================================================================
The series appears to be a time series of financial data, presumably one of a daily closing price of some financial instrumemt or index.

We observe that the time series is non-stationary in the mean. Therefore we can attempt to diff the time series to see if the resulting series is stationary in the mean. 
We also observe that the PACF of the time series indicates a correlation at lag 1 that ressembles and AR(1) series.

The plot of the original financial series does not indicate any amount of seasonality. The observation is confirmed by the ACF which doesn't display any significant amount of correlation at any lag other than the first.
We will therefore not try to fit a model that includes a seasonal component to the data.
We now proceed to analyze the first difference of the time series.

The 1st difference series is stationary in the mean. We also 
observe that after the first difference is taken, the ACF of the series, just like the PACF suggest white noise dynamics. But the first differential series does show clustered volatility in the variance.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#Load the data and describe it
data <- read.csv(file.path("lab3_series02.csv"))

#Describing Series
str(data)
summary(data)
cbind(head(data), tail(data))
quantile(as.numeric(data$DXCM.Close), c(0, .01,.05,.1,.25,.5,.75,.9,.95,.99, 1))
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#EDA on series
par(mfrow=c(2,2))
plot.ts(data$DXCM.Close, main="Financial Time Series",
        ylab="Value", xlab ="Time Units",
        col="blue")
hist(data$DXCM.Close, col="gray", main="Value")
acf(data$DXCM.Close, main="ACF of Time Series")
pacf(data$DXCM.Close, main="PACF of Time Series")
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# EDA on the first difference series
par(mfrow=c(2,2))
plot.ts(diff(data$DXCM.Close), main="First Difference Financial Time Series",
        ylab="Value", xlab ="Time Units",
        col="blue")
hist(diff(data$DXCM.Close), col="gray", main="Value")
acf(diff(data$DXCM.Close), main="ACF of Time Series")
pacf(diff(data$DXCM.Close), main="PACF of Time Series")
```

We now try to determine a best SARIMA model based on the best AIC for that series.
Interestingly enough, the best model based on AIC is a seasonal model with (p,d,q,P,D,Q) of values
(0, 0, 0, 0, 1, 0). Since we're assuming a frequency of one, that model is the same as the second best model, which has orders (p,d,q,P,D,Q) of (0, 1, 0, 0, 0, 0). We therefore decide to use (p,d,q,P,D,Q)=(0, 1, 0, 0, 0, 0) as our fitted model going forward.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#data.best <- get.best.sarima(data$DXCM, maxord=rep(3,6),1)
#data.best$best
#data.best$others[order(data.best$others$aics)[1:20],]
```

The fitted model is one with a differential component of value one, and has no other parameters for which we need to validate 95 confidence intervals.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Fitting a first difference series to our model
data.fit <- Arima(data$DXCM, order=c(0,1,0), seasonal=list(order = c(0,0,0)), method="CSS-ML")
data.res <- data.fit$resid
quantile(as.numeric(data.res), c(0, .01,.05,.1,.25,.5,.75,.9,.95,.99, 1))
t(confint(data.fit))
```

We now perform in-sample fit using the fitted series to assess our fitted model.
The fitted series models the original series very well and the model selection seems appropriate based on in-sample fit. 

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Performing in-sample fit using our fitted series
par(mfrow=c(1,1))
plot.ts(data$DXCM,col="navy",lty=2,
        main="Original vs a SAMIMA(0,1,0,0,0,0) Estimated Series with Residuals",
        ylab="Original and Estimate Values", ylim=c(0, 110))
par(new=T)
plot(fitted(data.fit),col="blue",axes=F,ylab="", ylim=c(0, 110))
leg.txt <- c("Original Series", "Estimated Series", "Residuals")
legend("topleft", legend=leg.txt, lty=c(2,1,2), 
       col=c("navy","blue","green"),
       bty='n', cex=1)
par(new=T)
plot.ts(data.res,axes=F,xlab="",ylab="",col="green", ylim=c(-10, 30), pch=1, lty=2)
axis(side=4, col="green")
mtext("Residuals", side=4, line=2,col="green")
```

To further assess the fitted series, We now perform 36 steps backtesting.
What the out of sample forecast shows is that the original series falls for the most part within the 80% confidence interval and almost totally within the 95% confidence interval of the prediction. 
However, we can see from the plot of the financial time series that the volatility of the series seems to increase over time. Clustered volatility in the variance likely explain this dynamic. We next turn our eyes to the residuals of the fitted series and to analysis of the dynamics of its variance.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Performing 36 steps backtesting using our fitted series
data.fit.back <- Arima(data$DXCM[1:(length(data$DXCM)-36)], 
                       order=c(0,1,0), seasonal=list(order = c(0,0,0)), method="CSS-ML")
summary(data.fit.back)
length(fitted(data.fit.back))
length(data.fit.back$resid)
df=cbind(data$DXCM[1:(length(data$DXCM)-36)], fitted(data.fit.back), data.fit.back$resid)
colnames(df)=c("orig_series", "fitted_vals", "resid")
head(df)

# Step 1: Plot the original and estimate series 
par(mfrow=c(1,1))
plot.ts(df[,"orig_series"], col="red", 
        main="Original vs a AR(1) Estimated Series with Residuals",
        ylab="Original and Estimated Values",
        xlim=c(0, 3000), ylim=c(0,110))
par(new=T)
plot.ts(df[,"fitted_vals"],col="blue",axes=T,xlab="",ylab="",
        xlim=c(0, 3000), ylim=c(0,110)) 
leg.txt <- c("Original Series", "Estimated Series", "Residuals")
legend("top", legend=leg.txt, lty=1, col=c("red", "navy", "green"),
       bty='n', cex=1)
par(new=T)
plot.ts(df[,"resid"],axes=F,xlab="",ylab="",col="green",
        xlim=c(0, 3000), ylim=c(-10,120), pch=1)
axis(side=4, col="green")
mtext("Residuals", side=4, line=2,col="green")

#Step 2: Out of sample forecast
data.fit.back.fcast <- forecast.Arima(data.fit.back, h=100)
length(data.fit.back.fcast$mean)
par(mfrow=c(1,1))
plot(data.fit.back.fcast,lty=2, col="navy",
     main="Out-of-Sample Forecast",
     ylab="Original, Estimated, and Forecast Values",
     xlim=c(0, 3000), ylim=c(0,110))
par(new=T)
plot.ts(data$DXCM,axes=F,lty=1, xlim=c(0, 3000), ylim=c(0,110), ylab="")
leg.txt <- c("Original Series", "Forecast series")
legend("top", legend=leg.txt, lty=1, col=c("black","blue"),
       bty='n', cex=1)
```

We observe from the residual time series that the variance of the series is non-stationary.  The series exhibits volatility with a variance changing in a regular way. It exhibits conditional heteroskedasticity. An observation of the PACF of the squared residuals series provides confirmation of the variance dynamics. Therefore, we decide to model its residuals using GARCH
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Plot the residuals time series
par(mfrow=c(3,2))
plot.ts(data.res, main="Residuals of Financial Time Series",
        ylab="Value", xlab ="Time Units",
        col="blue")
hist(data.res, col="gray", main="Value")
acf(data.res, main="ACF of Residuals")
pacf(data.res, main="PACF of Residuals")
acf(data.res^2, main="ACF of Squared Residuals")
```

We chose the default (p, q) = (1, 1) parameters of the function for our GARCH model. The parameters of the model are all significant based on a 95% confidence interval.

We observe from the ACF of the residuals of the GARCH fitted series that they have the charactersitics of white noise with mostly non-significant correlations at all lags of the ACF.
What the GARCH model of the residuals tells is that we can expect more or less volatility through the forecast of the point series that invalidate the confidence interval of our predictions since those were made with the assumption of a stationary variance.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Model the residuala of the financila time series using GARCH
data.garch <- garch(data.res, trace=F)
t(confint(data.garch))
data.garch.res <- resid(data.garch)[-1]

# Plot a histogram, ACF and PACF of the residuals after fitting a GARCH model
par(mfrow=c(2,2))
hist(data.garch.res, col="gray", main="Value")
acf(data.garch.res, na.action=na.pass, main="ACF of GARCH Residuals")
pacf(data.garch.res, na.action=na.pass, main="PACF of GARCH Residuals")
```

With the previous observations in mind, we still use the fitted series to predict 36 steps ahead. We will later adjust the confidence intervals of the predictions using our fitted GARCH model.

To perform a 36 steps ahead forecast of our series, we use the original point estimate of the series, that being the (0,1,0,0,0,0) SARIMA model initially estimated.
The GARCH model of the residuals will additionally be used to forcast the variance of the series, and help us adjust the confidence interval of the prediction.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#36 steps ahead sample forecast of the financial time series
data.fit.ahead.fcast <- forecast.Arima(data.fit, h=36)
length(data.fit.ahead.fcast$mean)
par(mfrow=c(1,1))
plot(data.fit.ahead.fcast,lty=2, col="navy",
     main="Out-of-Sample Forecast",
     ylab="Original, Estimated, and Forecast Values",
     xlim=c(0, 3000), ylim=c(0,110))
par(new=T)
plot.ts(data$DXCM,axes=F,lty=1, xlim=c(0, 3000), ylim=c(0,110),ylab="")
leg.txt <- c("Original Series", "Forecast series")
legend("top", legend=leg.txt, lty=1, col=c("black","blue"),
       bty='n', cex=1)
```

Having acknowledged the confidence interval problem on the prediction caused by the non-stationary variance of the financial search time series, we want to use our fitted GARCH model to predict the mean and variance of the residuals of the series.

The results of this prediction are a better estimate of the 95%confidence interval of the residuals of the global warming time series after modeling with our SAMIMA (1,1,1,0,0,0) model. We note that the volatility is predicted by the GARCH model to be in the range of -5 to 5, far wider than the predicted by the SARIMA model but consistent with the volatility observed towards the end of the original time series.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
data.garch.fit = garchFit(~ garch(1, 1), data = data.res, trace = FALSE)
data.garch.pred <- predict(data.garch.fit, n.ahead=36, plot=TRUE)
```

Using our GARCH model fitted on the residuals, we now plot the predicted confidence intervals 
obtained with GARCH modeling over the  original fitted SARIMA(0,1,0,0,0,0) series.
Replace the 95% confidence interval of the fitted SARIMA with the GARCH confidence interval
The mean series of the 36 steps ahead predctions obtained from the fitted first difference model are:
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
data.fit.ahead.fcast$mean
```
The corresponding lower and upper confidence intervals after substituting for the conditional variance obtained from GARCH model are:
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Reset the 95% confidence interval using the conditional variance from GARCH
data.fit.ahead.fcast$lower[,2] <- -(((data.fit.ahead.fcast$mean-data.fit.ahead.fcast$lower[,2])/(1.96*sqrt(data.fit.ahead.fcast$model$sigma2)))*data.garch.pred$standardDeviation)+data.fit.ahead.fcast$mean
data.fit.ahead.fcast$lower[,2]

data.fit.ahead.fcast$upper[,2] <- (((data.fit.ahead.fcast$upper[,2]-data.fit.ahead.fcast$mean)/(1.96*sqrt(data.fit.ahead.fcast$model$sigma2)))*data.garch.pred$standardDeviation)+data.fit.ahead.fcast$mean
data.fit.ahead.fcast$upper[,2]
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Clear the 80% confidence interval
data.fit.ahead.fcast$lower[,1] <- data.fit.ahead.fcast$mean
data.fit.ahead.fcast$upper[,1] <- data.fit.ahead.fcast$mean

# Plot the forecast with the updated 
par(mfrow=c(1,1))
plot(data.fit.ahead.fcast,lty=2, col="navy",
     main="Out-of-Sample Forecast",
     ylab="Original, Estimated, and Forecast Values",
     xlim=c(0, 3000), ylim=c(0,110))
par(new=T)
plot.ts(data$DXCM,axes=F,lty=1, xlim=c(0, 3000), ylim=c(0,110),ylab="")
leg.txt <- c("Original Series", "Forecast series")
legend("top", legend=leg.txt, lty=1, col=c("black","blue"),
       bty='n', cex=1)
par(new=T)
```

\newpage

Part 3 (25 points): Forecast the Web Search Activity for global Warming
=======================================================================
**Data Analysis**

 1. The time series has weekly 630 values starting at 1/4/04 and ending at 1/24/16.  The minimum value is -0.551 and the maximum value is 4.104.
 2. Time series plot shows that the series is very persistent,  The series is basically flat from 2004 to 2012.  After 2012, there is a sharp trend upward.  There is more volatility after 2012.  There are spikes and dips which could be seasonal with a yearly frequency.  The series is not stationary.
 3. Histogram shows is heavily positively skewed with most values between -0.551 and -0.3.
 3. ACF of the series has correlations at around 0.75 for almost 25 lags.
 4. PACF drops off immediately after first lag.  There are 4 points that fall outside the 95% confidence interval (blue lines) at lags 3, 5, 11 and 14.

**Model Selection Process**

1. __Try AR models.__  Use the ar() command in R to find AR(p) models or order p that potentially fit the time series.  This command output a model or order 15, but looking at the difference in AICs, the AIC for the AR(1) model is not that different from the AIC of the AR(15), so for parsimony we will try using that one.  Check if the residuals look like white noise.
- Histogram:  Yes.  This looks like a normal distribution.
- Fitted vs. Residuals:  No.  The plot does not look like an evenly distributed cloud.
- Plot:  No.  The plot does not look random, there is a lot of volatility on the right hand side of the graph.
- ACF:  No.  The ACF drops off after lag 0, but has only a few lags where the correlation comes out of the 95% CI.
- PACF:  No.  The PACF shows correlation with several values outside of the 9%% CI.
In summary, the residuals for this model do not look like white noise, so there is more variation that could be explained by our model.\newline
The In-Sample fit of this estimated model matches the original model very well as evidenced in the plot.

2. __Try ARIMA models.__  Use the get.best.arima() function which will try models with c(p,d,q) where p=0-4, d=0-2 and q=0-2.  And then we can print out a list in ascending order by AIC of the 20 models with the lowest AIC.  Inspecty these models for parsimony and select one with a good AIC and a small number of parameters.  The best model output from the function had an AIC of -1058.794 with parameters = c(1, 2, 2).  For parsimony a model of c(1,1,1) was chosen with an AIC of -1032.364 which is not that different from the best AIC.  Check if the residuals look like white noise.  No, the residuals do not look like white noise.  They exhibit the same characteristics as the AR(1) model from step 1.\newline
The In-Sample fit of this estimated model matches the original model very well as evidenced in the plot.

3. __Try SARIMA models.__  From the plot of the original series, it looks like this series has a seasonal component with a 52-week periodicity.  Use the get.best.sarima() function with parameters c(2,2,2,2,2,2).  The best AIC output is -1276.817 with a model of c(1, 2, 2, 1, 0, 2).  For parsimony try running get.best.sarima() with c(1,1,1,1,1,1).  A parsimonious model from this output is c(0, 1, 1, 1, 0, 1) with AIC -1246.412 which is very close to the AIC output from c(2,2,2,2,2,2).  For parsimony we will choose c(0, 1, 1, 1, 0, 1) and check the residuals.  No, the residuals do not look like white noise.  They exhibit the same characteristics as the AR(1) model from step 1.  The residuals exhibit evidence of time\newline
The In-Sample fit of this estimated model matches the original model very well as evidenced in the plot.

4. __Backtesting.__

5. __Forecast the model.__  Using the SARIMA model, we will make the requested 12-step ahead forecast of the model.  The forecast looks like it captures the seasonality of the model as it matches the upward trend and the seasonal volatility.

6. __GARCH.__ Since the residuals look like they have time-varying volatility, we will use GARCH to model the residuals.  The ACF of the squared residuals shows no obvious patterns or significant values. The model has reached and acceptable fit to the original time series.

7. __Update the forecast with GARCH.__ To add the GARCH information to the forecast, we create a GARCH forecast.  We then take the forecast from the SARIMA model and update it with the GARCH confidence intervals.

**Conclusion**

This time series is satisfactorily modeled with a SARMIMA(x,x,x,x,x,x) model to hande trends and seasonality and a GARCH model to handle the time-varying volatility.  We observe that the forecasts of the model are consisent with the seasonality of the original series and the forecasts are within the 95% confidence interval produced by GARCH.  This gives us confidence that this model will give us decent forecasts for the original time series.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Read in the time series data
glob.warm = read.csv("globalWarming.csv", header=TRUE)
glob.warm.ts = ts(glob.warm$data.science, start=2004, frequency = 52)
#glob.warm.ts = ts(glob.warm$data.science)
# Print descriptive statistics
str(glob.warm.ts)
summary(glob.warm.ts)
cbind(head(glob.warm.ts), tail(glob.warm.ts))
quantile(as.numeric(glob.warm.ts), c(.01,.05,.1,.25,.5,.75,.9,.95,.99))
# Plot the time series
plot.time.series(glob.warm.ts, 50, "Global Warming")

### 1. Try AR models
glob.warm.ar = estimate.ar(glob.warm.ts)
glob.warm.ar1=arima(glob.warm.ts, order=c(1,0,0))
# Plot the residuals
plot.residuals.ts(glob.warm.ar1, "AR(1)")
# Plot the In-sample fit
plot.orig.model.resid(glob.warm.ts, glob.warm.ar1, "AR(1)", c(2004,2016), c(-0.7,4))

### 2. Try ARIMA models
#gw.arima.best <- get.best.arima(glob.warm.ts, maxord=c(4,2,2))
# Print the top 20 best models based on AIC
#gw.arima.best$others[order(gw.arima.best$others$aics)[1:20],]
glob.warm.arima=arima(glob.warm.ts, order=c(1,1,1))
# Plot the residuals
plot.residuals.ts(glob.warm.arima,"ARIMA(1,1,1)")
# Plot the In-sample fit
plot.orig.model.resid(glob.warm.ts, glob.warm.arima, "ARIMA(1,1,1)", c(2004,2016), c(-0.7,4))

### 3. Try SARIMA models
#gw.seas.best <- get.best.sarima(glob.warm.ts, maxord=c(2,2,2,2,2,2), 52)
# Print the top 20 best models based on AIC
#gw.seas.best$others[order(gw.seas.best$others$aics)[1:20],]

#gw.seas.best1 <- get.best.sarima(glob.warm.ts, maxord=c(1,1,1,1,1,1), 52)
# Print the top 20 best models based on AIC
#gw.seas.best1$others[order(gw.seas.best1$others$aics)[1:20],]
glob.warm.arima.seas = arima(glob.warm.ts, order = c(0,1,1),
                             seas = list(order = c(1,0,1), 52),
                             method="CSS")
# Plot the residuals
glob.warm.arima.seas.res = glob.warm.arima.seas$residuals
plot.residuals.ts(glob.warm.arima.seas,"SARIMA(0,1,1,1,0,1)")
par(mfrow=c(1,1))
acf(glob.warm.arima.seas.res^2)
# Plot the In-sample fit
plot.orig.model.resid(glob.warm.ts, glob.warm.arima.seas, "SARIMA(0,1,1,1,0,1)", c(2004,2016), c(-0.7,4))

### 4. Backtesting
glob.warm.bt.ts = ts(glob.warm.ts[1:(length(glob.warm.ts)-52)],start=2004,frequency = 52)
glob.warm.arima.seas.bt = arima(glob.warm.bt.ts, order = c(0,1,1),
                             seas = list(order = c(1,0,1), 52), method="CSS")
df = cbind(glob.warm.bt.ts, 
           fitted(glob.warm.arima.seas.bt),
           glob.warm.arima.seas.bt$resid)
colnames(df) = c("orig_series", "fitted_vals", "resid")
head(df)
# Plot the original and estimate series
par(mfrow = c(1, 1))
plot.ts(df[, "orig_series"], col = "red", main = "Original vs SARIMA Estimated Series with Residuals",
        ylab = "Original and Estimated Values", xlim = c(2004, 2016),
        ylim = c(-.7,4))
par(new = T)
plot.ts(df[, "fitted_vals"], col = "blue", axes = T, xlab = "",
        ylab = "", xlim = c(2004, 2016), ylim = c(-.7,4))
leg.txt <- c("Original Series", "Estimated Series", "Residuals")
legend("top", legend = leg.txt, lty = 1, col = c("red", "navy",
                                                 "green"), bty = "n", cex = 1)
par(new = T)
plot.ts(df[, "resid"], axes = F, xlab = "", ylab = "", col = "green",
        xlim = c(2004, 2016), ylim = c(-.7,4), pch = 1)
axis(side = 4, col = "green")
mtext("Residuals", side = 4, line = 2, col = "green")

glob.warm.arima.bt.fcast = forecast.Arima(glob.warm.arima.seas.bt, h=52)
par(mfrow = c(1, 1))
plot(glob.warm.arima.bt.fcast, lty = 2, col = "navy", main = "Out-of-Sample Forecast",
     ylab = "Original, Estimated, and Forecast Values",
     xlim = c(2004, 2016), ylim = c(-.7, 4))
par(new = T)
plot.ts(glob.warm.ts, axes=F, lty = 1, col = "black", xlim = c(2004, 2016),
        ylim = c(-.7, 4), ylab = "")
leg.txt <- c("Original Series", "Forecast series")
legend("top", legend = leg.txt, lty = 1, col = c("black", "blue"),
       bty = "n", cex = 1)

### 5. Forecast the model
glob.warm.arima.fcast = forecast.Arima(glob.warm.arima.seas, h=12)
plot.model.forecast(glob.warm.arima.seas, glob.warm.arima.fcast,
                    "12", c(2004,2016), c(-0.7,5.5))

### 6. GARCH
glob.warm.garch.fit = garchFit(~ garch(1, 1), data = glob.warm.arima.seas.res, trace = FALSE)
gw.garch.pred <- predict(glob.warm.garch.fit, n.ahead=12, plot=TRUE)
glob.warm.garch.res <- glob.warm.garch.fit@residuals
par(mfrow=c(1,2))
acf(glob.warm.garch.res)
acf(glob.warm.garch.res^2)

### 7. Update the forecast with GARCH
# Clear the 80% confident interval of the fitted ARIMA by setting all values to the predicted mean of it
# And set the values of the 95% confidence interval to those that came out of the GARCH model
glob.warm.arima.fcast$lower[,1] <- glob.warm.arima.fcast$mean
glob.warm.arima.fcast$upper[,1] <- glob.warm.arima.fcast$mean
glob.warm.arima.fcast$lower[,2] <- gw.garch.pred$lowerInterval+glob.warm.arima.fcast$mean
glob.warm.arima.fcast$upper[,2] <- gw.garch.pred$upperInterval+glob.warm.arima.fcast$mean

plot.model.forecast(glob.warm.arima.seas, glob.warm.arima.fcast,
                    "12", c(2004,2016), c(-0.7,5.5))

```

Part 4 (25 points): Forecast Inflation-Adjusted Gas Price
==========================================================

The dataframe contains three variables. Date, Production and Price. It consists of 410 observations of those variables. The Date variable indicates that the data ranges from January 01 1978 to February 01 2012. We now perform some exploratory data analysis of those variables.

1 __Variable Production__ The histogram shows a data distribution of the variable that appears to be multimodal. We can certainly not assume that the underlying data comes from a normal distribution. However, there are no outliers or singularities in the data that would require that we investigate further or that we remove them from the data set.

2 __Variable Price__ The histogram shows a data distribution that appears to e positively skewed. There are no indications from the histogram that the data would follow a normal distribution. However, there are no outliers or singularities in the data that would require that we investigate further or that we remove them from the data set. Our side by side plot of both series shows trends up and down and volatility. The series appear to be non-stationary in the mean.
 

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Load the data
gas.data <- load(file.path('gasOil.Rdata'))

# Sumamry information about the data
str(gasOil)
summary(gasOil)
cbind(head(gasOil$Date), head(gasOil$Price), head(gasOil$Production), 
      tail(gasOil$Date), tail(gasOil$Price), tail(gasOil$Production))

# EDA for variable Production
print(quantile(gasOil$Production, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of at 15 bins
gasOil.prod.hist <- ggplot(gasOil, aes(Production)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(gasOil$Production)[2] - range(gasOil$Production)[1])/15) +
    labs(title = "Oil Production from 01/01/1978 to 02/01/2012", x = "Oil Production", y = "Frequency")

plot(gasOil.prod.hist)

# EDA for variable Price
print(quantile(gasOil$Price, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of at 15 bins
gasOil.price.hist <- ggplot(gasOil, aes(Price)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(gasOil$Price)[2] - range(gasOil$Price)[1])/15) +
    labs(title = "Oil Price from 01/01/1978 to 02/01/2012", x = "Oil Price", y = "Frequency")

plot(gasOil.price.hist)

# Side by side ynamics of Price and Production time series
# First convert the price data to a time series:
price.ts <- ts(gasOil$Price, start=c(1978,1), end=c(2012,2), frequency=12)
# Convert the production data to a time series
production.ts <- ts(gasOil$Production, start=c(1978,1), end=c(2012,2), frequency=12)

# Plot the two time series
par(mfrow=c(1,1))
plot.ts(price.ts, main="Oil Prices and Production From 1978 to 2012",
        ylab="Price and Production", xlab ="Time Units",
        col="navy")
par(new=T)
plot.ts(production.ts, ylab="", xlab="",
        col="green", axes=F)
axis(side=4, col="green")
leg.txt <- c("Oil Prices", "Oil Production")
legend("top", legend=leg.txt, lty=1, col=c("navy", "green"),
       bty='n', cex=1)

```

Task1
-----
We can assume that the AP tested the correlation of the time series of Oil Price and Oil Production. We can replicate the calculation of the reported p-value with a test of the correlation of the
variables Price and Production. The test reports a p-value of 0.5752, which is non-significant. The reported 95% confidence interval for the correlation is [-0.06927648  0.12427029]. Since the p-value for the test is non-significant, the confidence interval non-surprisingly spans the zero value.

When computing the correlation of two sets of observations of data, we assume that the data is from random samples drawn from the same population with a distribution that has a constant mean and variance. 
We know that our data is from a time time series. We have seen from the side by side plots that it's non-stationary in the mean. Therefore the assumption of constant mean in the calculation of the correlation does not hold and the calculated p-value is likely flawed.
Another assumtion made with correlations is the assumption of independence of variables in the samples. As stated before, we usually assume that the samples are random draws from a population. For a time series the assumption of independence between the data observations must be rejected. For times series, the observation of $x_{t}$ is dependent on previous observations of $x_{t−1}$, $x_{t−2}$,.... 
That dependency is captured in a joint probability distribution which is unavailable to us, as the time series represents the single instance of the realisation of a stochastic process that we are unable to observe.

We next turn to studying the time series of gas prices.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
cor.test(gasOil$Price, gasOil$Production)
```

Task 2
-------
The series exhibits trends up and down and a lot of volatility. It appears to be non-stationary in the mean.
We can also see that ACF is gradually descending, indicating a possible ARIMA or SARIMA dynamics. Knowing the series to be that of oil prices, we can speculate that it incorporates seasonality as we'd expect prices to follow the seasons of the year. We would expect yearly seasonality. The PACF shows significant correlations at lags 1 and possibly 2, suggesting that the series might have characteristics of an AR(1) or AR(2) component. 
To verify our observations, we next study 1 and 2 order differences of the series.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#EDA on series
par(mfrow=c(2,2))
plot.ts(price.ts, main="Oil Prices From 1978 to 2011 Time Series",
        ylab="Value", xlab ="Time Units",
        col="blue")
hist(price.ts, col="gray", main="Value")
acf(price.ts, main="ACF of Time Series")
pacf(price.ts, main="PACF of Time Series")
```

As expected, the lag(1) and lag(2) series are stationary in the mean. They appear to show conditional volatility that will need to be analyzed with an GARCH model.
We can also observe patterns that appear to be seasonal patterns of repetition on yearly basis in the series. That observation would support the ituition that weather cycles and corresponding consuption changes may affect gas prices.
Because the first difference series is stationary in the mean, we will not need the second order differencing of the series when we study it furter. 
We can also see that the ACF of the first and second order series drop sharply after lag 1 indicating the presence of an MA(1) component in the series. Similarly, the PACF of the first and second difference series have a single significant correlation at lag 1, indicating the possible presence of an AR(1) component in the series. We next perform a systematic search of the best model fit for the series based on the AIC.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
par(mfrow=c(2,2))
plot.ts(diff(price.ts), main="First order difference Oil Prices series",
        ylab="Value", xlab ="Time Units",
        col="blue")
hist(diff(price.ts), col="gray", main="Value")
acf(diff(price.ts), main="ACF of Time Series")
pacf(diff(price.ts), main="PACF of Time Series")

par(mfrow=c(2,2))
plot.ts(diff(price.ts, lag=2), main="Second order difference Oil Prices series",
        ylab="Value", xlab ="Time Units",
        col="blue")
hist(diff(price.ts, lag=2), col="gray", main="Value")
acf(diff(price.ts, lag=2), main="ACF of Time Series")
pacf(diff(price.ts, lag=2), main="PACF of Time Series")
```

The best SARIMA model fitted is a (p,d,q,P,D,Q) of (0, 1, 2, 1, 0, 3) with an AIC of -672.50. We do take note of the fact that immediately following that series, is a SARIMA of parameters (0, 1, 1, 1, 0, 3) with an AIC of -672.19.
With parsimony in mind, we select that model to fit our data and proceed to asses the in-sample fit of that model. It matches our earlier observations of a stationary first difference series, and of the possible presence of AR(1) and MA(1) components in the time series, along with that of a seasonal component.
An inspection of the confidence intervals of the parematers of this model indicate that the parameter for the second MA component of the seasonal model is not significant. But the third parameter of the same seasonal component is significant.
We next perform in-sample fit using the fitted series to assess our fitted model.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#price.best <- get.best.sarima(price.ts, maxord=rep(3,6))
#price.best$best
#price.best$others[order(price.best$others$aics)[1:20],]
```

The fitted series models the original series very well and the model selection seems appropriate based on in-sample fit.
To further assess the fitted series, We next perform 48 steps backtesting.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

# Fi the selected ARIMA model to the time series data.
price.fit <- Arima(price.ts, order=c(0,1,1), seasonal=list(order = c(1,0,3), period=12), method="CSS-ML")
price.res <- price.fit$resid
t(confint(price.fit))
quantile(as.numeric(price.res), c(0, .01,.05,.1,.25,.5,.75,.9,.95,.99, 1))


# We now perform in-sample fit using the fitted series to assess our fitted model.
par(mfrow=c(1,1))
plot.ts(price.ts,col="navy",lty=2,
        main="Original vs a SAMIMA(0, 1, 1, 1, 0, 3) Estimated Series with Residuals",
        ylab="Original and Estimate Values", ylim=c(1, 5))
par(new=T)
plot(fitted(price.fit),col="blue",axes=F,ylab="", ylim=c(1, 5))
leg.txt <- c("Original Series", "Estimated Series", "Residuals")
legend("topleft", legend=leg.txt, lty=c(2,1,2), 
       col=c("navy","blue","green"),
       bty='n', cex=1)
par(new=T)
plot.ts(price.res,axes=F,xlab="",ylab="",col="green", ylim=c(-1, 5), pch=1, lty=2)
axis(side=4, col="green")
mtext("Residuals", side=4, line=2,col="green")
```

Our out of sample forecast appears to be reasonable. The 80 and 95 % confidence intervals of the forecast partially include the actual values of the series. We had previously observed that the variance of the residuals seemed to show some volatility that we know is not modeled by the fitted point series.
Therefore, we next turn our eyes to the residuals of the fitted series and to the analysis of the dynamics of its variance.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
price.fit.back <- Arima(price.ts[1:(length(price.ts)-48)], 
                     order=c(0,1,1), seasonal=list(order = c(1,0,3), period=12), 
                     method="CSS-ML")
summary(price.fit.back)
length(fitted(price.fit.back))
length(price.fit.back$resid)
df=cbind(price.ts[1:(length(price.ts)-48)], fitted(price.fit.back), price.fit.back$resid)
colnames(df)=c("orig_series", "fitted_vals", "resid")
head(df)
# Step 1: Plot the original and estimate series 
par(mfrow=c(1,1))
plot.ts(df[,"orig_series"], col="red", 
        main="Original vs a AR(1) Estimated Series with Residuals",
        ylab="Original and Estimated Values",
        ylim=c(1,5))
par(new=T)
plot.ts(df[,"fitted_vals"],col="blue",axes=T,xlab="",ylab="",
        ylim=c(1,5)) 
leg.txt <- c("Original Series", "Estimated Series", "Residuals")
legend("top", legend=leg.txt, lty=1, col=c("red", "navy", "green"),
       bty='n', cex=1)
par(new=T)
plot.ts(df[,"resid"],axes=F,xlab="",ylab="",col="green",
        ylim=c(-1,5), pch=1)
axis(side=4, col="green")
mtext("Residuals", side=4, line=2,col="green")

#Step 2: Out of sample forecast
price.fit.back.fcast <- forecast.Arima(price.fit.back, h=48)
length(price.fit.back.fcast$mean)
par(mfrow=c(1,1))
plot(price.fit.back.fcast,lty=2, col="navy",
     main="Out-of-Sample Forecast",
     ylab="Original, Estimated, and Forecast Values",
     ylim=c(1,5), ,axes=F)
par(new=T)
plot.ts(price.ts,axes=T,lty=1, ylim=c(1,5), ylab="")
leg.txt <- c("Original Series", "Forecast series")
legend("top", legend=leg.txt, lty=1, col=c("black","blue"),
       bty='n', cex=1)
```

The ACF and PACF of the residual series resemble those of a white noise series. But we observe from the squared residuals time series that the variance of the series is non-stationary. 
The series exhibits volatility with a variance changing in a regular way. It exhibits conditional heteroskedasticity behavior. Therefore, we will to model its residuals using GARCH.
With this in mind, we proceed to an initial 48 steps ahead forecast of the gas price series.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Plot the residuals time series
par(mfrow=c(3,2))
plot.ts(price.res, main="Residuals of Oil Prices Series",
        ylab="Value", xlab ="Time Units",
        col="blue")
hist(price.res, col="gray", main="Value")
acf(price.res, main="ACF of Residuals")
pacf(price.res, main="PACF of Residuals")
acf(price.res^2, main="ACF of Squared Residuals")
```

The point estimates of our forecasts look reasoanble. We are aware that the model used for this point forecast assumes a stationary mean and variance. We can assess a stationary mean, but not a variance. The consequence of teh non-stationary variance is that the confidence intervals around our estimates are biased.
Having acknowledged the confidence interval problem on the prediction caused by the non-stationary variance of the financial search time series, we want to use our fitted GARCH model to predict the mean and variance of the residuals of the point series.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
#2012-2016 steps ahead sample forecast
price.fit.ahead.fcast <- forecast.Arima(price.fit, h=48)
length(price.fit.ahead.fcast$mean)
par(mfrow=c(1,1))
plot(price.fit.ahead.fcast,lty=2, col="navy",
     main="Out-of-Sample Forecast",
     ylab="Original, Estimated, and Forecast Values",
     ylim=c(1,5))
par(new=T)
plot.ts(price.ts,axes=F,lty=1, ylim=c(1,5),xlim=c(1978,2016), ylab="")
leg.txt <- c("Original Series", "Forecast series")
legend("top", legend=leg.txt, lty=1, col=c("black","blue"),
       bty='n', cex=1)
```

We observe from the ACF of the residuals of the GARCH fitted series that they have characteristics of white noise with mostly non-significant correlations at all lags of the ACF. What the GARCH model of the residuals tells is that we can expect more or less volatility through the forecast of the point series. 
That volatility affects the confidence intervals of the estimates of our SARIMA model as previously observed with the backstesting.
Using our fitted GARCH model, we can now better predict the variance of the point estimates from 2012 to 2016.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Fit a GARCH model to the residuals of the fitted time series
price.garch <- garch(price.res, order=c(1,1), trace=F)
t(confint(price.garch))
price.garch.res <- resid(price.garch)[-1]

# Perform EDA on residuals of fit
par(mfrow=c(2,2))
hist(price.garch.res, col="gray", main="Value")
acf(price.garch.res, na.action=na.pass, main="ACF of GARCH Residuals")
pacf(price.garch.res, na.action=na.pass, main="PACF of GARCH Residuals")

# Predict the residuals variance 4 years ahead 
price.garch.fit <- garchFit(~ garch(1, 1), data = price.res, trace = FALSE)
par(mfrow=c(1,1))
price.garch.pred <- predict(price.garch.fit, n.ahead=48, plot=TRUE)
```

Having better estimates of the variance with GARCH, we now update our estimated SARIMA model's confidence interval with those.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# Recompute the lower and upper 95% confidence intervals based on GARCH variance
price.fit.ahead.fcast$lower[,1] <- price.fit.ahead.fcast$mean
price.fit.ahead.fcast$upper[,1] <- price.fit.ahead.fcast$mean

price.fit.ahead.fcast$lower[,2] <- -(((price.fit.ahead.fcast$mean-price.fit.ahead.fcast$lower[,2])/(1.96*sqrt(price.fit.ahead.fcast$model$sigma2)))*price.garch.pred$standardDeviation)+price.fit.ahead.fcast$mean
price.fit.ahead.fcast$lower[,2]
price.fit.ahead.fcast$upper[,2] <- (((price.fit.ahead.fcast$upper[,2]-price.fit.ahead.fcast$mean)/(1.96*sqrt(price.fit.ahead.fcast$model$sigma2)))*price.garch.pred$standardDeviation)+price.fit.ahead.fcast$mean
price.fit.ahead.fcast$upper[,2]
```

We can now plot our 2012 to 2016 point estimates with the proper 95% confidence interval. 
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# 2012-2016 steps ahead sample forecast
par(mfrow=c(1,1))
plot(price.fit.ahead.fcast,lty=2, col="navy",
     main="Out-of-Sample Forecast",
     ylab="Original, Estimated, and Forecast Values",
     ylim=c(1,5))
par(new=T)
plot.ts(price.ts,axes=F,lty=1, ylim=c(1,5),xlim=c(1978,2016), ylab="")
leg.txt <- c("Original Series", "Forecast series")
legend("top", legend=leg.txt, lty=1, col=c("black","blue"),
       bty='n', cex=1)
```