---
title: "Lab2"
author: "Megan Jasek, Rohan Thakur, Charles Kekeh"
date: "Friday, March 04, 2016"
output: pdf_document
---

Question 1
===========
Part 1
------
$$E(Y|X) = \int_{0}^{x} y*\frac{1}{x} \ dy = \frac {y^{2}}{2x}|_{0}^{x} = \frac{x}{2} - 0$$
$$\mathbf {E(Y|X)=\frac{x}{2}}$$

Part 2
------
$$E(Y) = E(E(Y|X)) = E(\frac{x}{2}) = \frac{1}{2}E(x)$$
We know that,
$$f_{X}(x) = 1$$
Find E(X) as follows,
$$E(X) = \int_{0}^{1}x*f_{X}(x) \ dx = \int_{0}^{1}x*1 \ dx = \frac{x^2}{2}|_{0}^{1} = \frac{1}{2} - 0 = \frac{1}{2}$$
Substituting in for E(x) we get,
$$E(Y) = \frac{1}{2} * \frac{1}{2} = \frac{1}{4}$$
$$\mathbf {E(Y)=\frac{1}{4}}$$

Part 3
------
$$f_{X,Y}(x,y) = f_{Y|X}(y|x) * f_{X}(x)$$
We know that
$$f_{Y|X}(y|x) = \frac{1}{x} \ and \ f_{X}(x) = 1$$
Substituting these values in to the equation, we get
$$\mathbf { f_{X,Y}(x,y) = \frac{1}{x} }$$

Part 4
------
$$f_{Y}(y) = \int_{y}^{1}f_{Y|X}(y|x) * f_{X}(x) dx = \int_{y}^{1} \frac{1}{x}*1 \ dx$$
$$= \log(x)|_{y}^{1} = \log(1) - \log(y) = 0 - \log(y) = \log(\frac{1}{y})$$
$$f_{Y}(y) = \log(\frac{1}{y})$$
We know that
$$f_{X,Y}(x,y) = f_{X|Y}(x|y) * f_{Y}(y)$$
Solving for $f_{X|Y}(x|y)$, we get
$$f_{X|Y}(x|y) = \frac {f_{X,Y}(x,y)}{f_{Y}(y)}$$
Substituting, we get
$$f_{X|Y}(x|y) = \frac {\frac{1}{x}}{\log(\frac{1}{y})}$$
$$\mathbf {f_{X|Y}(x|y) = \frac{1}{x\log(\frac{1}{y})}}$$

Part 5
------
$$E(X|Y=\frac{1}{2})= \int_{\frac{1}{2}}^{1} x*\frac{1}{x\log(2)} dx = \frac{1}{\log(2)}  \int_{\frac{1}{2}}^{1} 1 \ dx$$ 
$$=\frac{1}{\log(2)} * (x|_\frac{1}{2}^{1}) = \frac{1}{\log(2)} * (1 - \frac{1}{2})$$
$$=\frac{1}{\log(2)} * \frac{1}{2}=\frac{1}{2\log(2)}$$
$$\mathbf{E(X|Y=\frac{1}{2})=\frac{1}{2\log(2)}}$$

Question 2
===========
$$Payoff\ function = aA + bB + cC$$
Let us calculate the variance of the payoff.

$$Var(Payoff) = Var(aA + bB + cC)$$
$$={a}^{2}Var(A) + {b}^{2}Var(B) + {c}^{2}Var(C)$$ 
Since A, B an C are independent, all covariance terms are 0. \newline
Now, using the relation Var(A)=2Var(B)=3Var(C): 
$$=6{a}^{2}Var(C) + \frac{3}{2}{b}^{2}Var(C) + {c}^{2}Var(C)$$

We can clearly see from this equation, that in order to minimize variance, all the allocation must be in asset C, since any allocation in A or B, leads to a higher variance than the same allocation in C.\newline
**Final answer: (a,b,c) = (0,0,1)**

Question 3
===========
$y_{i}, i={1,\cdots,n}$ random uniform variables.

Part 1 - Likelihood Function
----------------------------
$L(\theta)$ being the likelihood function, we know we have:
$$L(\theta)=f(y_{1},\cdots,y_{n}|\theta)=f(y_{1}|\theta)f(y_{2}|\theta)\cdots f(y_{n}|\theta)$$

Where f is the uniform probablity density function with parameter $\theta$.
$$f(y_{i},\theta) = \left\{ \begin{array}{ll}
\frac{1}{\theta} & \mbox{ for $0 \leq y_{i} \leq \theta$} \\
0 & \mbox{otherwise}
\end{array}
\right.$$

Making
$$L(\theta) = \left\{ \begin{array}{ll}
\frac{1}{\theta^{n}} & \mbox{for $0 \leq y_{i} \leq \theta, i \in {1,\cdots,n} $} \\
0 & \mbox{otherwise}
\end{array}
\right.$$

Part 2 - MLE
----------------------------
Based on $L(\theta)$ The MLE of $\theta$ is a value of $\theta$ for which $\theta \geq y_{i} for i \in{1,\cdots,n}$ and which maximizes $1/\theta^{n}$.
$MLE(\theta)$ is the smallest of such values of $\theta$ such that $\theta \geq y_{i}$ for $i \in {1,\cdots,n}$. Therefore:
$$\mathbf{MLE(\theta) = \hat{\theta} = max(y_{1},\cdots,y_{n})}$$

Part 3 - Expectation n=1
------------------------
Taking $\hat{\theta} = max(y_{1},\cdots,n)$ and n=1\newline
We have
$$\hat{\theta} = y_{1}$$
And
$$\mathbf{E[\hat{\theta}] =  E[y_{1}] = \frac{\theta}{2}}$$
Knowing that $y_{i}$ is from a random uniform distribution over $[0,\theta]$

Part 4 - Bias
-------------
Yes, from the above, $\hat{\theta}$ is biased. For any ${y_{1},\cdots,y_{n}}$, we expect $max{y_{1},\cdots,n} < \theta$ with probability 1. Hence $\hat{\theta}$ underestimates $\theta$ and we have just proven that for n=1, $E(\hat{\theta}) \neq \theta$.

Part 5 - Expectation general case
---------------------------------
Taking $\hat{\theta}=max(y_{1},\cdots,y_{n})$ and assuming $n \geq 1$.
$$E(\hat{\theta}) = E[max(y_{1},\cdots,y_{n})]$$

Let's define $x = max(y_{i}), i \in {1,\cdots,n}$.
$$CDF(x) = P(max(y_{i},\cdots,y_{n}) < x), i \in {1,\cdots,n}$$
$$CDF(x) = P(y_{1} < x, y_{2} < x, \cdots, y_{n} < x)$$
$$CDF(x) = \prod{P(y_{i}<x)}, i \in {1,\cdots,n}$$
$$CDF(x) = (\frac{x}{\theta})^{n}$$

From CDF(x), which is the cumulative distribution of x, we deterne the desnsity probability as
$$PDF(x) = \frac{\delta}{\delta x} (\frac{\theta}{x})^{n}$$
$$PDF(x) = \frac{n x^{n-1}}{\theta^n}$$
From PDF(x), we can now compute E(x) as:
$$E(x) = \int_{x=0}^{\theta} \frac{n (x^{n-1})}{\theta} xdx$$
and
$$\mathbf{E(x) = \hat{\theta} = \frac{n}{n+1}\theta}$$

Part 6 - Expectation general case
---------------------------------
From the previous compututation of the general case of $n \geq 1$, we can state that
$$\mathbf{\lim_{n \rightarrow \infty} \hat{\theta} = \theta}$$
and $\hat{\theta}$ is a consistent estimator of $\theta$.

Question 4
==========
4.1  Univariate Analysis
------------------------
- __wage__ - The wage variable has a range from $127 to $2,404 with a mean of $579 and median of $543 with most values occuring between $250 and $750.  The histogram shows a data distribution that's positevely skewed.  There are a few large outliers.  Taking the log of this variable would have the outliers take on values that are much less extreme in relation to the other variable's values.
- __logWage__ - The logWage variable has a range from $4.844 to $7.785 with a mean of $6.263 and median of $6.297.  The histogram shows a data distribution that's approximately normal.
- __education__ - The education variable is an integer and is discrete and has a range from 2 to 18 with a mean of 12 and median of 12.  The histogram shows a data distribution that is slightly negatively skewed.  There is a spike at 12 and a smaller spike at 16.
- __experience__ - The experience variable is an integer and is discrete and has a range from 0 to 23 with a mean of 8.788 and median of 8.  The histogram shows a data distribution where more values lie at the lower end of the distribution.
- __experienceSquare__ - The experience variable is an integer and is discrete and has a range from 0 to 529 with a mean of 95.03 and median of 64.  The histogram shows a data distribution where more values lie at the lower end of the distribution.  There is a spike at about 50.  This variable has a large outlier, which is amplified by taking the square.
- __IQscore__ - The IQscore variable is an integer and is discrete and has a range from 50 to 144 with a mean of 102.3 and median of 103.  The histogram shows a data distribution that is approximately normal.  There are 316 missing values.
- __dad_education__ - The dad_education variable is an integer and is discrete and has a range from 0 to 18 with a mean of 10.18 and median of 11.  The histogram shows a data distribution that has many frequencies at about count 30 and spikes at 8 and 12.  These spikes make intuitive sense because these are natural educaiton breakpoints for people.  Eight years signifying the end of middle school and 12 years indicating the end of high school.  There are 239 missing values.
- __mom_education__ - The mom_education variable is an integer and is discrete and has a range from 0 to 18 with a mean of 10.45 and median of 12.  The histogram shows a data distribution that has many frequencies at about count 50 and spikes at 12.  This spike makes intuitive sense because 12 years indicates the end of high school which is a natural education break point for people.  There are 128 missing values.
- __age__ - The age variable is an integer and is discrete and has a range from 24 to 34 with a mean of 28.01 and median of 27.  For the ages between 24 and 28, the fequency is around 105.  For the ages between 29 and 34, the frequency is around 65.
- __raceColor__ - The raceColor variable is a binary variable with values 0 or 1 and mean 0.238. This means that there are about 24% 1's and 76% 0's.
- __rural__ - The rural variable is a binary variable with values 0 or 1 and mean 0.391. This means that there are about 39% 1's and 61% 0's.  39% of the participants live in a rural area and 61% do not.
- __city__ - The rural variable is a binary variable with values 0 or 1 and mean 0.712. This means that there are about 71% 1's and 29% 0's.  71% of the participants live in a city and 29% do not.
- __z1__ -  The z1 variable is a binary variable with values 0 or 1 and mean 0.44. This means that there are about 44% 1's and 56% 0's.
- __z2__ -  The z2 variable is a binary variable with values 0 or 1 and mean 0.686. This means that there are about 69% 1's and 31% 0's.

```{r,include=FALSE}
library(car)
library(ggplot2)
library(sandwich)
library(lmtest)
library(pastecs)
library(ivpack)
library(knitr)
library(psych)
opts_chunk$set(tidy.opts=list(width.cutoff=70))
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Load the data in to the df dataframe
data = read.csv("WageData2.csv", header=TRUE)
# There was already a logWage variable in the dataset, so set that one to logWageOLD
data$logWageOLD = data$logWage
# Create a logWage variable to use for the rest of the problem
data$logWage = log(data$wage)
# Create the experienceSquare variable
data$experienceSquare = data$experience * data$experience
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# wage variable
summary(data$wage)
print(quantile(data$wage, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of apps at 30 bins
wage.hist <- ggplot(data, aes(wage)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$wage)[2] - range(data$wage)[1])/30) +
    labs(title = "Distribution of wage", x = "wage ($)", y = "Frequency")

plot(wage.hist)

# logWage variable
summary(data$logWage)
print(quantile(data$logWage, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of apps at 30 bins
logWage.hist <- ggplot(data, aes(logWage)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$logWage)[2] - range(data$logWage)[1])/30) +
    labs(title = "Distribution of logWage", x = "logWage ($)", y = "Frequency")

plot(logWage.hist)

# education variable
summary(data$education)
print(quantile(data$education, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of apps at 30 bins
education.hist <- ggplot(data, aes(education)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$education)[2] - range(data$education)[1])/30) +
    labs(title = "Distribution of education", x = "education (years)", y = "Frequency")

plot(education.hist)

# experience variable
summary(data$experience)
print(quantile(data$experience, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of apps at 30 bins
experience.hist <- ggplot(data, aes(experience)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$experience)[2] - range(data$experience)[1])/30) +
    labs(title = "Distribution of experience", x = "experience (years)", y = "Frequency")

plot(experience.hist)

# experienceSquare variable
summary(data$experienceSquare)
print(quantile(data$experienceSquare, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of apps at 30 bins
experienceSquare.hist <- ggplot(data, aes(experienceSquare)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$experienceSquare)[2] - range(data$experienceSquare)[1])/30) +
    labs(title = "Distribution of experienceSquare", x = "experienceSquare (years^2)", y = "Frequency")

plot(experienceSquare.hist)

# IQscore variable
summary(data$IQscore)
print(quantile(data$IQscore, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
IQscore.hist <- ggplot(data, aes(IQscore)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black") +
    labs(title = "Distribution of IQscore", x = "IQscore (points)", y = "Frequency")

plot(IQscore.hist)

# dad_education variable
summary(data$dad_education)
print(quantile(data$dad_education, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
dad_education.hist <- ggplot(data, aes(dad_education)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black") +
    labs(title = "Distribution of dad_education", x = "dad_education (years)", y = "Frequency")

plot(dad_education.hist)

# mom_education variable
summary(data$mom_education)
print(quantile(data$mom_education, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
mom_education.hist <- ggplot(data, aes(mom_education)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black") +
    labs(title = "Distribution of mom_education", x = "mom_education (years)", y = "Frequency")

plot(mom_education.hist)

# age variable
summary(data$age)
print(quantile(data$age, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
age.hist <- ggplot(data, aes(age)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$age)[2] - range(data$age)[1])/30) +
    labs(title = "Distribution of age", x = "age (years)", y = "Frequency")

plot(age.hist)

# raceColor variable
summary(data$raceColor)
print(quantile(data$raceColor, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
raceColor.hist <- ggplot(data, aes(raceColor)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$raceColor)[2] - range(data$raceColor)[1])/30) +
    labs(title = "Distribution of raceColor", x = "raceColor", y = "Frequency")

plot(raceColor.hist)

# rural variable
summary(data$rural)
print(quantile(data$rural, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
rural.hist <- ggplot(data, aes(rural)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$rural)[2] - range(data$rural)[1])/30) +
    labs(title = "Distribution of rural", x = "rural", y = "Frequency")

plot(rural.hist)

# city variable
summary(data$city)
print(quantile(data$city, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
city.hist <- ggplot(data, aes(city)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$city)[2] - range(data$city)[1])/30) +
    labs(title = "Distribution of city", x = "city", y = "Frequency")

plot(city.hist)

# z1 variable
summary(data$z1)
print(quantile(data$z1, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
z1.hist <- ggplot(data, aes(z1)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$z1)[2] - range(data$z1)[1])/30) +
    labs(title = "Distribution of z1", x = "z1", y = "Frequency")

plot(z1.hist)

# z2 variable
summary(data$z2)
print(quantile(data$z2, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1),na.rm=TRUE))

# Plot the histogram of apps at 30 bins
z2.hist <- ggplot(data, aes(z2)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$z2)[2] - range(data$z2)[1])/30) +
    labs(title = "Distribution of z2", x = "z2", y = "Frequency")

plot(z2.hist)
```

4.2  Bivariate Analysis
-----------------------
- __wage, logWage vs. education__ - Both wage and logWage are weakly correlated with education with a correlation value of about 0.3.  The wage vs. education scatterplot shows a possible linear trend.
- __wage, logWage vs. experience__ - Both wage and logWage appear uncorrelated with experience with very low correlation values of  -0.0060 and -0.0290, respectively.  The wage vs. experience scatterplot shows that experience is not affected by wage for the most part.  The logWage vs. experience scatterplot shows that experience is not affected by logWage as well.
- __wage, logWage vs. experienceSquare__ - Both wage and logWage appear uncorrelated with experienceSquare with very low correlation values of -0.043 and -0.065, respectively.  The wage vs. experienceSquare scatterplot shows that experienceSquare is not affected by wage for the most part.  The logWage vs. experienceSquare scatterplot shows that experience is not affected by logWage as well.
- __wage, logWage vs. IQscore__ - Both wage and logWage are weakly correlated with IQscoare with low correlation values of 0.186 and 0.201, respectively.  The wage and logWage vs. IQscore scatterplots show that IQscoare affects wage and logWage slightly.  As wage or logWage go up, IQscore increases by a small amount.
- __wage, logWage vs. dad_education__ - Both wage and logWage are weakly correlated with dad_education with low correlation values of 0.19 and 0.19, respectively.  The wage and logWage vs. dad_education scatterplots show that dad_education affects wage and logWage slightly.  As wage or logWage go up, dad_education increases by a small amount.
- __wage, logWage vs. mom_education__ - Both wage and logWage are weakly correlated with mom_education with low correlation values of 0.20 and 0.21, respectively.  The wage and logWage vs. mom_education scatterplots show that mom_education affects wage and logWage slightly.  As wage or logWage go up, mom_education increases by a small amount.
- __wage, logWage vs. age__ - Both wage and logWage are weakly correlated with age with low correlation values of 0.26 and 0.25, respectively.  The wage and logWage vs. age scatterplots show that age affects wage and logWage slightly.  As wage or logWage go up, age increases by a small amount.
- __wage, logWage vs. raceColor__ - Both wage and logWage are weakly correlated with raceColor with low correlation values of -0.30 and -0.34, respectively.  The wage and logWage vs. raceColor scatterplots show that raceColor affects wage and logWage slightly.  As wage or logWage go up, there are fewer people that have the raceColor variable set to 1.
- __wage, logWage vs. rural__ - Both wage and logWage are weakly correlated with rural with low correlation values of -0.22 and -0.25, respectively.  The wage and logWage vs. rural scatterplots show that rural affects wage and logWage slightly.  As wage or logWage go up, there are fewer people that have the rural variable set to 1.
- __wage, logWage vs. city__ - Both wage and logWage are weakly correlated with city with low correlation values of 0.22 and 0.24, respectively.  The wage and logWage vs. rural scatterplots show that city affects wage and logWage slightly.  As wage or logWage go up, there are more people that have the city variable set to 1.
- __wage, logWage vs. z1__ -  Both wage and logWage are weakly correlated with z1 with low correlation values of 0.101 and 0.087, respectively.  The wage and logWage vs. z1 scatterplots show that z1 affects wage and logWage slightly.  As wage or logWage go up, there are more people that have the z1 variable set to 1.
- __wage, logWage vs. z2__ - Both wage and logWage are weakly correlated with z2 with low correlation values of 0.17 and 0.18, respectively.  The wage and logWage vs. z2 scatterplots show that z2 affects wage and logWage slightly.  As wage or logWage go up, there are more people that have the z2 variable set to 1.  z2 shows a slightly stronger correlation with wage and logWage than z1.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Scatter plot with wage variable
wage.education.plot = ggplot(data, aes(y = wage, x = education)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. Education", y = "wage ($)", x = "education")
plot(wage.education.plot)
# Scatter plot with logWage variable
lwage.education.plot = ggplot(data, aes(y = logWage, x = education)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. Education", y = "logWage ($)", x = "education")
plot(lwage.education.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$education)
cor(data$logWage, data$education)

# Scatter plot with wage variable
wage.experience.plot = ggplot(data, aes(y = wage, x = experience)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. experience", y = "wage ($)", x = "experience")
plot(wage.experience.plot)
# Scatter plot with logWage variable
lwage.experience.plot = ggplot(data, aes(y = logWage, x = experience)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. experience", y = "logWage ($)", x = "experience")
plot(lwage.experience.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$experience)
cor(data$logWage, data$experience)

# Scatter plot with wage variable
wage.experienceSquare.plot = ggplot(data, aes(y = wage, x = experienceSquare)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. experienceSquare", y = "wage ($)", x = "experienceSquare")
plot(wage.experienceSquare.plot)
# Scatter plot with logWage variable
lwage.experienceSquare.plot = ggplot(data, aes(y = logWage, x = experienceSquare)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. experienceSquare", y = "logWage ($)", x = "experienceSquare")
plot(lwage.experienceSquare.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$experienceSquare)
cor(data$logWage, data$experienceSquare)

# Scatter plot with wage variable
wage.IQscore.plot = ggplot(data, aes(y = wage, x = IQscore)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. IQscore", y = "wage ($)", x = "IQscore")
plot(wage.IQscore.plot)
# Scatter plot with logWage variable
lwage.IQscore.plot = ggplot(data, aes(y = logWage, x = IQscore)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. IQscore", y = "logWage ($)", x = "IQscore")
plot(lwage.IQscore.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$IQscore, use="complete.obs")
cor(data$logWage, data$IQscore, use="complete.obs")

# Scatter plot with wage variable
wage.dad_education.plot = ggplot(data, aes(y = wage, x = dad_education)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. dad_education", y = "wage ($)", x = "dad_education")
plot(wage.dad_education.plot)
# Scatter plot with logWage variable
lwage.dad_education.plot = ggplot(data, aes(y = logWage, x = dad_education)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. dad_education", y = "logWage ($)", x = "dad_education")
plot(lwage.dad_education.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$dad_education, use="complete.obs")
cor(data$logWage, data$dad_education, use="complete.obs")

# Scatter plot with wage variable
wage.mom_education.plot = ggplot(data, aes(y = wage, x = mom_education)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. mom_education", y = "wage ($)", x = "mom_education")
plot(wage.mom_education.plot)
# Scatter plot with logWage variable
lwage.mom_education.plot = ggplot(data, aes(y = logWage, x = mom_education)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. mom_education", y = "logWage ($)", x = "mom_education")
plot(lwage.mom_education.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$mom_education, use="complete.obs")
cor(data$logWage, data$mom_education, use="complete.obs")

# Scatter plot with wage variable
wage.age.plot = ggplot(data, aes(y = wage, x = age)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. age", y = "wage ($)", x = "age")
plot(wage.age.plot)
# Scatter plot with logWage variable
lwage.age.plot = ggplot(data, aes(y = logWage, x = age)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. age", y = "logWage ($)", x = "age")
plot(lwage.age.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$age)
cor(data$logWage, data$age)

# Scatter plot with wage variable
wage.raceColor.plot = ggplot(data, aes(y = wage, x = raceColor)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. raceColor", y = "wage ($)", x = "raceColor")
plot(wage.raceColor.plot)
# Scatter plot with logWage variable
lwage.raceColor.plot = ggplot(data, aes(y = logWage, x = raceColor)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. raceColor", y = "logWage ($)", x = "raceColor")
plot(lwage.raceColor.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$raceColor)
cor(data$logWage, data$raceColor)
by(data$wage, data$raceColor, describe)
by(data$logWage, data$raceColor, describe)

# Scatter plot with wage variable
wage.rural.plot = ggplot(data, aes(y = wage, x = rural)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. rural", y = "wage ($)", x = "rural")
plot(wage.rural.plot)
# Scatter plot with logWage variable
lwage.rural.plot = ggplot(data, aes(y = logWage, x = rural)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. rural", y = "logWage ($)", x = "rural")
plot(lwage.rural.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$rural)
cor(data$logWage, data$rural)
by(data$wage, data$rural, describe)
by(data$logWage, data$rural, describe)

# Scatter plot with wage variable
wage.city.plot = ggplot(data, aes(y = wage, x = city)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. city", y = "wage ($)", x = "city")
plot(wage.city.plot)
# Scatter plot with logWage variable
lwage.city.plot = ggplot(data, aes(y = logWage, x = city)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. city", y = "logWage ($)", x = "city")
plot(lwage.city.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$city)
cor(data$logWage, data$city)
by(data$wage, data$city, describe)
by(data$logWage, data$city, describe)

# Scatter plot with wage variable
wage.z1.plot = ggplot(data, aes(y = wage, x = z1)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. z1", y = "wage ($)", x = "z1")
plot(wage.z1.plot)
# Scatter plot with logWage variable
lwage.z1.plot = ggplot(data, aes(y = logWage, x = z1)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. z1", y = "logWage ($)", x = "z1")
plot(lwage.z1.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$z1)
cor(data$logWage, data$z1)
by(data$wage, data$z1, describe)
by(data$logWage, data$z1, describe)

# Scatter plot with wage variable
wage.z2.plot = ggplot(data, aes(y = wage, x = z2)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "Wage vs. z2", y = "wage ($)", x = "z2")
plot(wage.z2.plot)
# Scatter plot with logWage variable
lwage.z2.plot = ggplot(data, aes(y = logWage, x = z2)) +
  theme(legend.position = "none") +
  geom_point(colour = "Blue") + 
  geom_jitter(colour = "Blue") +
  geom_smooth(colour = "red", method = "lm") +
  labs(title = "LogWage vs. z2", y = "logWage ($)", x = "z2")
plot(lwage.z2.plot)
# Run correlations with wage and logWage variables
cor(data$wage, data$z2)
cor(data$logWage, data$z2)
by(data$wage, data$z2, describe)
by(data$logWage, data$z2, describe)
```

4.3  Regress log(wage) on education, experience, age, and raceColor
-------------------------------------------------------------------
Part 1
------
Report all the estimated coefficients, their standard errors, t-statistics,
F-statistic of the regression, R2, adjustedR2, and degrees of freedom\newline
The requested information is shown in the summary information below.\newline

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
OLS.logWage.educ.exper.age.race = lm(logWage ~ education + experience + age + raceColor, data=data)
summary(OLS.logWage.educ.exper.age.race)
```
Part 2
------
Degress of freedom = 996.  This value is calculated from the following formula $df = n - k - 1$ where n is the number of observations (n=1000).  k is the number of independent varialbes (k=3).  Plugging in these values we get, $996 = 1000 - 3 - 1$.  k=3 because the age variable is not used in the regression even though it is in the formula because it is a linear combination of the other variables.

Part 3
------
The unexpected results from the regression are that the age variable has coeffiecient estimates that are NA.  This is because age is a linear combination of the education and experience variables as expressed by the formula $age = education + experience +6$.  To resolve this issue one of these 3 varialbes needs to be removed from the regression.  Since the intent is to estimate return to education on race and experience, then the age variable can be removed.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Create a new variable that represents the linear combination of age with education and experience.
data$age.formula = data$education + data$experience + 6
# Show that this new variable isdataeed the same as the age variable to subtracting the two variables.
data$age.difference = data$age -data$age.formula
# Now in the summary of the difference variable, all of the values are 0 indicating that the age.formula variable
# is the same as the age variable.
summary(data$age.difference)
```
Part 4 - Interpret the coefficient estimate associated with education
---------------------------------------------------------------------
The estimate for the education coefficient is 0.079608.  This means that for every unit change in education, there is an 8.00% change in wage.  This value is significant at the 5% significance level.  This is a large practical effect.

Part 5 - Interpret the coefficient estimate associated with experience
----------------------------------------------------------------------
The estimate for the experience coefficient is 0.035372.  This means that for every unit change in experience, there is a 3.53% change in wage.  This value is significant at the 5% significance level.  This is a large practical effect, yet it's only about half as big as the effect from education.

Question 4.4
------------
Part 1
------
See graph below of the estimated effect of experience on wage.
$$\frac {\delta logWage}{\delta experience} = 0.0924 - 2*(0.00288)*experience$$

Part 2
------
$$dlogWage10 = 0.0924 - 2*(0.00288)*10 = 0.0348$$
The estimated effect of experience on wage when experience is 10 years is 3.48%.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Create the model
OLS.logWage.educ.exper.exper2.race = lm(logWage ~ education + experience + experienceSquare + raceColor, data=data)
# Print the summary of the model
summary(OLS.logWage.educ.exper.exper2.race)

# Create a variable dlogWage the represents the line created by the change in logWage with respect to a change
# in experience
dlogWage=0
for (experience in 1:30){
  dlogWage[experience] = 0.0924 - 2*(0.00288)*experience
}
# Graph the line
plot(dlogWage, lty="dashed", main="Estimated Effect of Experience on logWage",
        col="blue", ylab="logWage", xlab="Experience")

# Calculate the value of the effect of experience on wage when experience is 10 years.
dlogWage10 = 0.0924 - 2*(0.00288)*10
dlogWage10
```

Question 4.5
------------
Part 1
------
The number of observations used in this regression 723 (out of 1,000).  The participants with missing mom_education or dad_education values (Group 1) compare to participants that have both a mom_education and a dad_education value (Group 2) as follows.\newline
- __wage__ - Group 1 participants have lower median and mean wages than the Group 2 participants.  The median and mean for values for Group 1 are $481 and $570 respectively vs. $531 and $597 respectively for Group 2.  The standard deviation of Group 1 wages is lower, at 256.9 vs. 268.1 for Group 2. The T-test for difference of means between the 2 groups is significant at the 1% level.\newline
- __education__ - Group 1 participants have lower median and mean education than the Group 2 participants.  The median and mean for values for Group 1 are 12 and 12.1 respectively vs. 13.7 and 13.7 respectively for Group 2.  The standard deviation of Group 1 education is higher, at 2.7 vs. 2.6 for Group 2. The T-test for difference of means between the 2 groups is significant at the 1% level.\newline
- __experience__ - Group 1 participants have higher median and mean experience than the Group 2 participants.  The median and mean for values for Group 1 are 10 and 10.5 respectively vs. 8 and 8.2 respectively for Group 2.  The standard deviation of Group 1 experience is higher, at 4.3 vs. 4.0 for Group 2. The T-test for difference of means between the 2 groups is significant at the 1% level.   \newline
- __raceColor__ - Group 1 participants have a disproportianately larger number of participants with raceColor=1, at 45%, vs. 16% for Group 2. The T-test for difference of means between the 2 groups is significant at the 1% level.\newline

Part 2
------
We do not think we can just throw away the participants with the missing values.  They are important to the analysis since they represent a disproportional amount of people with lower wages, less education, more experience and higher proportion of raceColor variables equal to 1 than participants without missing values. These differences are statistically significant.

Part 3
------
This is not a good idea, because averages can be skewed by outliers. Since neither dad_education nor mom_education are evenly distributed, they will inevitably be skewed, hence interfering with our regression, letting outliers have even more influence than they already have on regressions.

Part 4
------
This is a bad idea because we are introducing multicollinearity into the regression and losing precision of our coefficients.

Part 5
------
We certainly cannot use the regression models with missing values replaced. Both these techniques lead to highly non-significant coefficients for mom_education and dad_education, meaning that coefficients obtained for these variables cannot be trusted. At the same time, having 277 values missing is not acceptable since our original regression misses a lot of important data for variables for which we do have information. Therefore, we would not elect to go with any of the models from the given choices.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Part 1
# Create the model
OLS.logWage.8var = lm(logWage ~ education + experience + experienceSquare + raceColor + dad_education + mom_education + rural + city, data=data)
# Print the model
summary(OLS.logWage.8var)

# Creating a dummy variable for rows with missing values
data$missingval = is.na(data$mom_education) | is.na(data$dad_education)
summary(data$missingval)

#Now, we check the variables by the missing value dummy variable.
#Additionally, we check whether there is difference in the two groups by running a t-test
#We do this for wage, education, experience and raceColor
by(data$wage, data$missingval, describe)
t.test(data[data$missingval,c("wage")], data[!data$missingval,c("wage")])

by(data$education, data$missingval, describe)
t.test(data[data$missingval,c("education")], data[!data$missingval,c("education")])

by(data$experience, data$missingval, describe)
t.test(data[data$missingval,c("experience")], data[!data$missingval,c("experience")])

by(data$raceColor, data$missingval, describe)
t.test(data[data$missingval,c("raceColor")], data[!data$missingval,c("raceColor")])

# Part 3
# Copy the dataset to a new variable
data.avgForNA = data
# Set all of the values with dad_education = NA to the mean of dad_education
data.avgForNA$dad_education[is.na(data.avgForNA$dad_education)] = mean(data.avgForNA$dad_education, na.rm=TRUE)
# Set all of the values with mom_education = NA to the mean of mom_education
data.avgForNA$mom_education[is.na(data.avgForNA$mom_education)] = mean(data.avgForNA$mom_education, na.rm=TRUE)
# Rerun the regression
OLS.logWage.8var.avgNA = lm(logWage ~ education + experience + experienceSquare + raceColor + dad_education + mom_education + rural + city, data=data.avgForNA)

# Part 4
# Copy the dataset to a new variable
data.regressForNA = data
# Regress dad_education on the education, experience and raceColor variables
m1 = lm(dad_education ~ education + experience + raceColor, data=data)
# Regress mom_education on the education, experience and raceColor variables
m2 = lm(mom_education ~ education + experience + raceColor, data=data)

# Set all of the values with dad_education = NA to the value output from using the
# regression coefficients from m1 above.
data.regressForNA$dad_education[is.na(data.regressForNA$dad_education)] = m1$coefficients[1] +
  m1$coefficients[2]*data.regressForNA$education + m1$coefficients[3]*data.regressForNA$experience +
  m1$coefficients[4]*data.regressForNA$raceColor
# Set all of the values with mom_education = NA to the value output from using the
# regression coefficients from m2 above.
data.regressForNA$mom_education[is.na(data.regressForNA$mom_education)] = m2$coefficients[1] +
  m2$coefficients[2]*data.regressForNA$education + m2$coefficients[3]*data.regressForNA$experience +
  m2$coefficients[4]*data.regressForNA$raceColor

# Rerun the regression
OLS.logWage.8var.regressNA = lm(logWage ~ education + experience + experienceSquare + raceColor + dad_education + mom_education + rural + city, data=data.regressForNA)

# Part 5
# Print the summaries of the 2 new models
summary(OLS.logWage.8var.avgNA)
summary(OLS.logWage.8var.regressNA)
```

Question 4.6
------------
Part 1
------
The assumptions needed are as follows:\newline
1. Cov(z1, education) != 0.  z1 needs to have some correlation with the variable, education, for which it is an instrument.\newline
2. Cov(z1, u) = 0.  z1 cannot have any correlation with the error term.

Part 2
------
Suppose z1 is an indicator representing whether or not an individual lives in an area in which there was a recent policy change to promote the importance of education.  Yes, z1 could be correlated with other unobservables captured in the error term.  Some examples are 1. Income.  People with higher incomes might be more educated and thus might place a higher importance on eduction and thus be more likely to live in an area that promotes education, 2.  Political party.  A particular political party might be more aligned with education and therefore people in that polical party might be more inclined to live in an area that promotes education, and 3.  Whether you voted or not.  It's possible that people who vote might be more educated and more likely to live in an area that promotes education.  These are just a few examples.  There could be many more.

Part 3
------
Using the same specification as that in question 4.5, estimate the equation by 2SLS, using both z1 and z2 as instrument variables.\newline

The coefficient estimate on education goes from 0.0681701 in the original model to 0.075490, however, in the new model, the education estimate is not significant at the 5% level, so the increase in the coefficient can no longer be used in our interpretation.\newline


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Run the IV TSLS regression with z1 and z2
TSLS.logWage.8var = ivreg(logWage ~ education + experience + experienceSquare + raceColor + dad_education + mom_education + rural + city | z1 + z2 + experience + experienceSquare + raceColor + dad_education + mom_education + rural + city, data=data)
# Print the summary of TSLS the model
summary(TSLS.logWage.8var)
# Print the summary of the original model
summary(OLS.logWage.8var)
```

Question 5
==========

Part 1
------
In order to come up with our parsimonious model, we first examined the dataset. We found high correlation between urb and lit, and therefore chose not to use those in order to prevent the negative effects of multicollinearity in our results. Since the research question is concerned with voteshare and absolute_wealth, we choose a simple univariate model with the dependent variable as voteshare and the dependent variable as absolute wealth. However, from examining this variable, it is clear that it is heavily positively skewed, and therefore requires a log transformation. Additionally, some cleanup is required, removing coded values.

Final Parsimonious Model: y = voteshare, x = absolute_wealth

Results from regression: Our model is statistically significant at the 1% level. We can interpret the coefficient as saying a 1% increase in absolute wealth corresponds to a 0.005% increase in votes
Answering Research Question: Wealthy candidates fare very slightly better in elections. There is a linear relationship, but with a very small slope, such that it is almost flat.


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
dataset=read.csv('wealthy_candidates.csv')
#Exploring dataset
describe(dataset$absolute_wealth)
cor(dataset[,c("urb", "lit", "voteshare", "absolute_wealth")], use ="pairwise.complete.obs")

#Now, examining abs wealth variable
summary(dataset$absolute_wealth)
print(quantile(dataset$absolute_wealth, probs = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9,0.95, 0.99, 1), na.rm=TRUE))
hist(dataset$absolute_wealth, breaks = 60)

head(dataset[order(dataset$absolute_wealth, decreasing = TRUE), c("absolute_wealth")])
#We can see that this variable is highly skewed. In order continue using the variable,
#we must remove coded values like 2, and transform the variable to log.
dataset$absolute_wealth_clean = log(dataset$absolute_wealth) 
dataset$absolute_wealth_clean[dataset$absolute_wealth_clean==log(2)] = NA 
print(quantile(dataset$absolute_wealth_clean, probs = c(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9,0.95, 0.99, 1), na.rm=TRUE))
summary(dataset$absolute_wealth_clean)
hist(dataset$absolute_wealth_clean, breaks = 60)

#Now, to start building the regression model
votes.plot = ggplot(dataset, aes(x = absolute_wealth_clean, y = voteshare)) + theme(legend.position = "none") +
  geom_point(colour = "Blue") + geom_smooth(colour = "red", method = "lm") + labs(title = "Voteshare vs. Log(Absolute Wealth)", x = "log Absolute Wealth ($)", y = "Vote Share")
plot(votes.plot)

#Does not seem like much of a relation, but we continue on to run the regression.
model = lm(voteshare~absolute_wealth_clean,data=dataset)
summary(model)
```


Part 2
------
An addition of a quadratic term is absolutely unwarranted, and would only skew the original absolute wealth variable further.
For comparison purposes, we will create a new model with the wealth variable without the log, and with the square.

Result: Highly non-significant model and coefficients, cannot reject the null.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Creating a clean variable without the log
dataset$absolute_wealth_clean2 = dataset$absolute_wealth 
dataset$absolute_wealth_clean2[dataset$absolute_wealth_clean2==2] = NA 
dataset$absolute_wealth_clean2Square = dataset$absolute_wealth_clean2^2 
model2=lm(voteshare~absolute_wealth_clean2+absolute_wealth_clean2Square, data=dataset)
summary(model2)
```

Part 3
------
We run a new model with dummy variables for region 2 and region 3. With this model, we obtain statistical and practical significance of the dummy coefficients, as well as a substantial increase in the R squared value from the original parsimonious model.

Ater testing the difference in models, we obtain a significant wald test as well, showing that the region variables are clearly a good addition to the model.
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
model3 = lm(voteshare~absolute_wealth_clean+region,data=dataset)
summary(model3)
summary(model)
waldtest(model, model3, vcov=vcovHC)
```

Part 4
------
In our parsimonious model, our errors are endogenous, leading to the omitted variable bias in our coefficient for log(absolute wealth). This is evident from the fact that when we add region, we see a drastic change in the coefficient for log(absolute wealth). Therefore, we cannot say that we have a causal, unbiased estimate, because we know our coefficient is biased.
Causality holds when we have (apart from MLR1-MLR4) 
1. Exogeneity of errors (which is volated in this case), and 
2. the ability to manipulate x to observe changes in y without affecting the error term. We could theoretically conceive of a situation where we find people following an ideal absolute wealth distribution and have them run for elections, observing the results. However, this is not practical in this case.


Part 5
------
$$Change\ in\ Voteshare = \beta_0 + \beta_1*(Change\ in\ log\ absolute\ wealth) + u$$
This model would yield a causal result when the error terms that are endogenous, are also time constant, and would therefore cancel out.

However, in our case, this model does not work for several reasons.
1. We do not have data across time periods.
2. If we assume that we do have data across time, one could argue that the variable absolute wealth is close to being time-constant, and would therefore mostly cancel out, which would mean we would lose our main independent variable.
3. Changes in absolute wealth and its affect on the change in votes does not help answer our original research question. A poorer candidate could have a larger change in wealth than a richer candidate, and we would lose this informaation by doing a difference model.

Question6
==========
Part 1 - Reorganizing the data
------------------------------
With the data loaded, we can see that it includes product data for a period of four years of 2004, 2005, 2006 and 2007. The organization of the data suggests that a transformation to wide form would make the analysis easier.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
load("retailSales.Rdata")
```

Part 2 - Variable analysis and establishing a population model
---------------------------------------------------------------
The variables in the dataset provided to us are: Year, Product.line, Product.type, Product, Order.method-type, Retailer.country, Revenue, Planned.revenue, Product.cost, Quantity, Unit.cost, Unit.price, Gross.profit and Unit.sale.price.
Without additional information about these variables, we want to know which ones would introduce multicollinearity in our model, should we decide to include them jointly and which ones may require transformations.

We want build a model to predict revenue from the variables available, using data from the first two years. We will build a model predicting revenues in 2005 from 2004 data and will validate that model using 2006 data to predict 2007 data.

Part 2.1 Unit.price and Unit.sale.price
----------------------------------------
The correlation between these two variables is 0.999275 indicating that only one of these two variables should be part of our model. We choose to drop the Unit.sale.price variable from consideration in our model.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
cor(retailSales$Unit.price, retailSales$Unit.sale.price, use="pairwise.complete.obs")
```

Part 2.2 Unit.price and Unit.cost
----------------------------------
The correlation between these variables has a value of 0.988687. We conclude that adding the two variables to our model will bring little more information than adding a single one. We choose to drop the Unit.cost from consideration in our model.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
cor(retailSales$Unit.price, retailSales$Unit.cost, use="pairwise.complete.obs")
```

Part 2.3 Gross.profit and Unit.price * Quantity
-----------------------------------------------
The correlation value between these terms is 0.9765178. We similarly conclude that incorporating all three variables in out model will add little more information to our model than the two more relevant. Because conceptually Gross.profit is a function of Quantity and Unit.price, we choose to drop Gross.profit from consideration in our model.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
cor(retailSales$Gross.profit, retailSales$Unit.price*retailSales$Quantity, use="pairwise.complete.obs")
```

Part 2.4 Product, Product.line and Product.type
-----------------------------------------------
An analysis of the data confirms that no product belongs to more than one product line or product type as expected. Therefore, we choose to omit the Product.line and Product.type from our prediction model, since they bring no more information than the identification of a product.

Part 2.5 Product.cost and Quantity * Unit.cost
----------------------------------------------
The correlation between these two terms is 0.9998837. Under that observation, we conclude that the Product.cost variable
would bring little information in the model beyond that which we would obtain after adding variables Quantity and Product.cost (or Product.Unit.price as an alternative variable that's highly correlated to it) 

Part 2.5 Population model
-----------------------------------------------
With the above analysis completed, the variables left for consideration when establishing a prediction model are:\newline
Product\newline
Order.method.type\newline
Retailer.country\newline
Unit.price\newline
Quantity\newline

Part 2.6 Revenue
----------------
Revenue is the dependent variable in the model. Before stating the population model with this variable, we take a look at a histogram and statistics about this variable.

An analysis of the Revenue variable shows that 59929 out of 84672 values of that variable are NAs. These will be ommited from the model.

A histogram of the Revenue variable indicates a large variance of the revenue numbers across products as is often the case for monetary figures. The result of the very large range is a very positively skewed distribution of revenues with 
most of the values in the smaller numbers and a long tail of products with very large revenue numbers.
In order to adjust the distribution of revenue, we choose to model the log of Revenue.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Remove entries with NA values from Revenue
retailSales.complete <- retailSales[!is.na(retailSales$Revenue),]

summary(retailSales.complete$Revenue)
stat.desc(retailSales.complete$Revenue)
print(quantile(retailSales.complete$Revenue, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))


# Plot the histogram of Revenue
revenue.hist <- ggplot(retailSales.complete, aes(Revenue)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black") +
    labs(title = "Distribution of Revenue", x = "Revenue ($)", y = "Frequency")

plot(revenue.hist)

retailSales.complete$logRevenue <- log(retailSales.complete$Revenue)

# Plot the histogram of log(Revenue)
log.revenue.hist <- ggplot(retailSales.complete, aes(logRevenue)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black") + 
                   labs(title = "Distribution of log(Revenue)", x = "log(Revenue) ($)", y = "Frequency")

plot(log.revenue.hist)
```

Part 2.7 Coming up with a population model
-------------------------------------
The previous sections have demonstrated very high multicollinearity in the data that, for most of the variables in the data set, consititutes a violation of MLR3.
It's worth considering MLR1 and MLR3 at this point.

The data in the set does not seem to originate from what we would call a population that seems homogeneous. The variety of product lines and product types, indicates disparity. We have one data sample for each product, sometimes across geographic locations, for every 4 year of data in the set. One cannot look at the data and claim that it represents a representative sample of some homogenous population, violating MLR2.

In addition, the remaining variables in the data set after exclusion of the highly correlated ones, are: product, revenue, retailer country and order method type. There is no theoretical foundation that allows us to establish that any of these variables can be used to model a linear relation with revenue across various products, thus violating MLR1.

For all these reasons, it seem reasonable not to attempt to model revenue from the data provided, since any model built from the data would violate all the assumptions of OLS.

Part 2.8 Hypothetical Population model
--------------------------------------
If we were to establish our population model using the remaining variables to predict the revenue of the year 2005 revenue based on the value of those variables in 2004.
Our population model could be:
  $$logRevenue.2005 = \beta_{0} + \beta{1}*Product + \beta_{2}*Order.method.type + \beta{3}*Retailer.country + \beta_{4}*Revenue.2004 + \beta_{5}*Quantity + u$$
  
The first step of our analysis would to transform the data using the reshape function such that  product information for the four years of 2004-2007 is represented on a single row.
 
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Reshape the data, bringing all 4 years of observations for each product into a single record,
#in wide format.
wideRetailSales <- reshape(
    retailSales.complete, timevar = "Year", 
    idvar= c("Product", 
             "Product.line", 
             "Product.type", 
             "Order.method.type",
             "Retailer.country"), 
    direction="wide")

projected.revenue.model <- lm(Revenue.2005 ~ Product + Retailer.country + 
    Order.method.type + Revenue.2004 + Quantity.2004, wideRetailSales)
summary(projected.revenue.model)
```

Part 3 Evaluating the model
---------------------------
For all the reasons listed in part 2.7, we've rejected the proposed population model and therefore skip its analysis as we believe it violates all OLS assumptions.
 
