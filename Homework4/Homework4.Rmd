---
title: "Homework4"
author: "Megan Jasek, Rohan Thakur, Charles Kekeh"
date: "Saturday, February 20, 2016"
output: pdf_document
---

```{r,include=FALSE}
library(car)
library(ggplot2)
library(sandwich)
library(lmtest)
library(pastecs)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=70))
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Load the dataset and print descriptions
load("athletics.RData")
desc
```

Question1
==========

How many observations and variables are in the dataset?
-------------------------------------------------------
There are 116 observations of 14 variables in the data set.

Data Analysis
-------------
- __apps__ - The histogram shows a data distribution that's positevely skewed with most universities showing between 3000 and 10000 applications for the years 1992 and 1993.
- __bowl__ - The bowl variable is a binary categorical variable. We note that over the 2 year period considered there are fewer universities that appear in bowl games than universities that do.
- __btitle__ - The btitle variable is also a binary categorical variable. The histogram, (as one would expect) shows that there are significantly fewer universities that have won a title over the 2 year period considered than universities that have.
- __finfour__ - The finfour variable is also a binary categorical variable. The histogram, (as one would expect) shows that there are significantly fewer universities that have participated in men's final four games over the 2 year period considered than universities that have.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#How many observations and variables are in the dataset
str(data)
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# apps variable
print(quantile(data$apps, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of apps at 15 bins
apps.hist <- ggplot(data, aes(apps)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$apps)[2] - range(data$apps)[1])/15) +
    labs(title = "Distribution of Applications", x = "Number of applications for admissions", y = "Frequency")

plot(apps.hist)
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# bowl variable
print(quantile(data$bowl, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of bowl at 15 bins
bowl.hist <- ggplot(data, aes(bowl)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$bowl)[2] - range(data$bowl)[1])/15) +
    labs(title = "Distribution of Bowl Game Participation", x = "Bowl game participation", y = "Frequency")

plot(bowl.hist)
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# btitle variable
print(quantile(data$btitle, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of btitle at 15 bins
btitle.hist <- ggplot(data, aes(btitle)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$btitle)[2] - range(data$btitle)[1])/15) +
    labs(title = "Distribution of btitle variable", x = "Conf Champ title Won", y = "Frequency")

plot(btitle.hist)
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# finfour variable
print(quantile(data$finfour, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of finfour at 15 bins
finfour.hist <- ggplot(data, aes(finfour)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(data$finfour)[2] - range(data$finfour)[1])/15) +
    labs(title = "Distribution of finfour variable", x = "Final-Four game participation", y = "Frequency")

plot(finfour.hist)
```

Question2
============
The distribution of the change of log of application number has the appearance of a
normal(ish) distribution. There are 2 outlier points with a change of -.2 and +.4
that correspond to Arizona University and Arkansas University. There are however
no indications that these outliers would affect the regression at this point or that
they should be removed.

Which schools had the greatest increase in number of log applications?
----------------------------------------------------------------------
Arizona, Alabama and Arizona State (see table below).

Which schools had the greatest decrease in number of log applications?
----------------------------------------------------------------------
Arkansas, Oklahoma State and Penn State (see table below).

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
reshaped.data <- reshape(data, v.names=c("apps","top25","ver500","mth500","stufac",
                                         "bowl","btitle","finfour","lapps",
                                         "avg500","bball","perf"),
                         timevar="year",idvar="school", direction="wide")
#Check the layout of the reshaped data
str(reshaped.data)

# Create the new variable for the change in the log of the number of applications
reshaped.data$clapps <- reshaped.data$lapps.1993-reshaped.data$lapps.1992

#examine the new variable
print(quantile(reshaped.data$clapps, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))

# Plot the histogram of at 20 bins
clapps.hist <- ggplot(reshaped.data, aes(clapps)) + 
    theme(legend.position = "none") + 
    geom_histogram(fill = "Blue", colour = "Black", binwidth = (range(reshaped.data$clapps)[2] - range(reshaped.data$clapps)[1])/20) +
    labs(title = "Distribution of clapps variable", x = "Change in number of log aplications", y = "Frequency")

plot(clapps.hist)
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Which schools had the greatest increase in number of log applications?
head(reshaped.data[order(reshaped.data$clapps, decreasing= TRUE), c("school", "clapps")])
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
# Which schools had the greatest decrease in number of log applications?
head(reshaped.data[order(reshaped.data$clapps, decreasing= FALSE), c("school", "clapps")])
```

Question3
==========
Which of these new variables has the highest variance?
------------------------------------------------------
The cperf variable has the highest variance, with a value of 0.82425892

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Create the additional variables
reshaped.data$cperf <- reshaped.data$perf.1993-reshaped.data$perf.1992
print(quantile(reshaped.data$cperf, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
print(stat.desc(reshaped.data$cperf))

reshaped.data$cbball <- reshaped.data$bball.1993-reshaped.data$bball.1992
print(quantile(reshaped.data$cbball, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
print(stat.desc(reshaped.data$cbball))

reshaped.data$cbowl <- reshaped.data$bowl.1993-reshaped.data$bowl.1992
print(quantile(reshaped.data$cbowl, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
print(stat.desc(reshaped.data$cbowl))

reshaped.data$cbtitle <- reshaped.data$btitle.1993-reshaped.data$btitle.1992
print(quantile(reshaped.data$cbtitle, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
print(stat.desc(reshaped.data$cbtitle))

reshaped.data$cfinfour <- reshaped.data$finfour.1993-reshaped.data$finfour.1992
print(quantile(reshaped.data$cfinfour, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
print(stat.desc(reshaped.data$cfinfour))
```

Question4
==========
a. Additional assumptions needed for the population model to be causal
----------------------------------------------------------------------
There are 2 additional assumptions that are needed for the proposed population model to be causal:\newline
\newline
__Assumption 1:__  Exogeneity of the dependent variables in the model with the error term in the model. Exogeneity can be defined as the requirement that the dependent variables are not related to the error term. The presence of omitted variables that are correlated with any of the variables in the model creates a bias in the estimation of the coefficients of the variables selected in the model, hence this assumption is important.

Assuming the population model presented:
$$lapps_{i}=\gamma_{0} + \beta_{0}I_{1993} + \beta_{1}bowl_{it} + \beta_{2}btitle_{it} + \beta_{3}finfour_{it} + a_{i} + u_{it}$$
Exogeneity of the variables in the model can be formulated as:
$$Cov(bowl_{it}, a_{i}+u_{it})=Cov(btitle_{it}, a_{i}+u_{it})=Cov(finfour_{it}, a_{i}+u_{it})=0$$

__Assumption 2:__ Manipulation.  We must assume that we have the ability to make manipulations to at least one of the x terms (the independent variables, but only one at a time) in order to observe changes in the y term (the dependent variable), without affecting the error term, so that we can estalish that there is no bias introduced.  In mathematical terms we can say that if we define our changes in x as $\delta x$ and our error term as $\epsilon$, then we have:
$$\delta x = f(x), and\ Cov(\delta x, \epsilon) = 0$$

Without this assumption, then we are talking about correlation and not causality.

b.  Additional assumption needed for OLS to consistently estimate the first-difference model
--------------------------------------------------------------------------------------------
For OLS to consistently estimate the first difference model, we need to make two additonal assumptions:\newline
\newline
__Assumption 1:__  Exogeneity as explained in part a.

Assuming the difference model presented:
$$clapps_{i}=\beta_{0} + \beta_{1}cbowl_{i} + \beta_{2}cbtitle_{i} + \beta_{3}cfinfour_{i} + u_{i}$$
Exogeneity of the variables in the model can be formulated as:
$$Cov(cbowl_{i}, u_{i})=Cov(cbtitle_{i}, u_{i})=Cov(cfinfour_{i}, u_{i})=0$$

__Assumption 2:__ As long as the effects of the omitted variables from the population model are constant, they don't affect the differences between the independent x variables from time 1 to time 2, so they will not affect the coefficients in the difference model.  If the effects of the omitted variables are not constant, then there will still be omitted-variable bias in the difference model.  So the assumption that we have to make for the difference model is that the effects of the omitted variables are constant over time.

It is reasonable to assume that anything that affects fin_four, cbtitle, bowl is probably a variable that has to do with the general sporting ability / quality of sports recruitment, which can be assumed to be constant over a year, so this additional assumption is plausible.

Question5
===========
Model Analysis
--------------
- __F-statistic__ - The F-statistic for the model has a p-value of 0.03855, which is significant at the 0.05 level.
- __intercept__ - The coefficient for the intercept is 0.01684 indicating that the year over year incease in application is 1.6% from 1992 to 1993.  However, the t-statistic for that coefficient has a value of 0.1932 and is not significant at the 0.05 level.
- __cbowl__ - The coefficient for the cbowl variable is .057, indicating that a win in a bowl the year prior, contributes to a 5.7% increase in applications the subsequent year.  The t-statistic for the coefficient is significant at the 0.05 level, with a value of 0.0236.
- __cbtitle__ - The coefficient for the cbtitle variable is .041, indicating that a win in the men's conference championship in the previous year, translates into an increase of 4.1% year over year from 1992 to 1993, however we cannot be sure of this relationship given the lack of statistical significance.  The t-statistic for the coefficient is 0.1950 and is not significant at the 0.05 level.
- __cfinfour__ - The coefficient for the cfinfour variable is -0.06961 and seems to indicate that an appearance in the men's final four the year prior is related to a decrease of 6.9% of applications, however we cannot be sure of this relationship given the lack of statistical significance. The t-statistic for the coefficient is 0.1348 and is not significant at the 0.05 level.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Create the model.
change.lapps.model <- lm(clapps ~ cbowl + cbtitle + cfinfour, data = reshaped.data)
#Estimate the model
print(summary(change.lapps.model))
```

Question6
==============
The F-statistic obtained from the model is the test of overall significance of the model. We have already establish that it has a p-value of 0.03855, which is significant at the 0.05 level.  We can obtain the same statistic with the linearHypothesis function.

Though the model is significant, we do not want to read too much into the t-statistics of each of the coefficients of the model. We had noted that 3 out of 4 coefficients had p-values that were not significant at the 0.05 level.  It appears that the combined explanatory power of the model is still relevant, as evidenced by the F-statistic.

Alternate second para (Rohan): RT: Since the model is significant, with an R squared value of 14%, we can infer that the independent variables do have some explanatory power over the dependent variable. However, we cannot be confident of relationships between the variables due to high standard errors and a lack of statistical significance of the coefficients.


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
linearHypothesis(change.lapps.model, c("cbowl", "cbtitle", "cfinfour"))
```
