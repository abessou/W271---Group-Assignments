---
title: "Homework 3"
author: "Rohan Thakur, Charles Kekeh and Megan Jasek"
date: "February 13, 2016"
output: pdf_document
---

```{r,include=FALSE}
library(ggplot2)
library(boot)
library(pastecs)
library(car)
library(knitr)
library(sandwich)
library(lmtest)
opts_chunk$set(tidy.opts=list(width.cutoff=50))
```

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Load the dataframe
load("twoyear.RData")
desc
```

Question 1
==========
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
summary(data$lwage)
print(quantile(data$lwage, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$lwage, 50, ylim=c(0,350))

summary(data$jc)
print(quantile(data$jc, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$jc, 50)

summary(data$univ)
print(quantile(data$univ, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$univ, 50, xlim=c(0,8))

summary(data$exper)
print(quantile(data$exper, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$exper, 50, xlim=c(0,200))

summary(data$black)
print(quantile(data$black, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$black, 50)

summary(data$hispanic)
print(quantile(data$hispanic, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$hispanic, 50)

summary(data$AA)
print(quantile(data$AA, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$AA, 50)

summary(data$BA)
print(quantile(data$BA, probs = c(.01, .05, .10, .25, .50, .75, .90, .95, .99, 1)))
hist(data$BA, 50, ylim=c(0,5000))
```

Basic structure of the data
---------------------------
There are no missing values in the data.\newline
__lwage__ variable has a normal-like distribution.\newline
__jc__ variable has values from 0 to about 4 and is heavily positively skewed with a
majority of values at or near 0.\newline
__univ__ variable has values from 0 to 7.5 and is heavily positively skewed with a majority of values at or near 0.\newline
__exper__ variable has values from 0 to 166 and is negatively skewed with a hill-climb distribution from 0 to about 500.\newline
__black, hispanic, AA, BA__ variables are binary with values of 0 or 1.\newline

Question 2
==========
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Create the experXblack variable by multiplying the exper and black variables.
data$experXblack = data$exper*data$black

# Run the requested OLS regression.
ols.lwage.8ind = lm(lwage~jc+univ+exper+black+hispanic+AA+BA+experXblack, data=data)
summary(ols.lwage.8ind)
# Print the diagnostic plots
plot(ols.lwage.8ind)

#Print the B_hat4 and B_hat8 coefficients
print(ols.lwage.8ind$coefficients["black"])
print(ols.lwage.8ind$coefficients["experXblack"])
```

Interpret the coefficients $\hat{\beta}4$ and $\hat{\beta}8$
------------------------------------------------
$\hat{\beta}4$ is the estimate for the black variable coefficient.\newline
$\hat{\beta}8$ is the estimate for the experXblack variable.\newline

The $\beta_{4}$ coefficient captures the effect of being black on the log of wage, 
holding all other variables in the model fix and assuming zero-employment experience. It provides an indication of how much percentage point change to expect in the wage of an individual when they move from the baseline (non-black) to being black. Hence the coefficient captures the difference at the intercept of the log(wage) vs experience plot between black and white respondants.

The $\beta_{8}$ coefficient captures the effect of being black on the impact of 
experience over the years on wage. It is better explained in terms of derivatives with:
$$\frac{\delta log(wage)}{\delta exper} = \beta_{3} + \beta_{8}*black$$
In the previous formulation, we can see that the $\beta_{8}$ coefficient captures the
impact of being back on the slope of the log(wage) vs experience curve. In other words, the coefficient describes how much more or less experience impacts the log of wage for
black people over the years, vs the baseline (non-black people).
And because the outcome variable is the log of wage, the impact above can actually be  formulated in terms of impact of ethnicity on percentage changes on the actual 
wage of individuals.

Question 3
==========
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Show the summary of the model again
summary(ols.lwage.8ind)

#Print the univ coefficient
print(ols.lwage.8ind$coefficients["univ"])
```

Test that the return to university education is 7%.
---------------------------------------------------
Null Hypothesis: H0:  $\beta2=0.07$.\newline
Alternate Hypothesis: H1:  $\beta2\neq0.07$.\newline
Using the linear model and summary statistics, we compute the pvalue for the test above as:

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
pvalue = 2* (1- pt ((ols.lwage.8ind$coefficients["univ"] - 0.07)/
        coef(summary(ols.lwage.8ind))[, "Std. Error"]["univ"], 
        df = summary(ols.lwage.8ind)$df[1]))
pvalue
```

Based on the p-value, the test is not significant at the 0.05% significance level.  Therefore, we can't reject the null hypothesis that the return to university education is 7%.

Question 4
==========

Test that the return to junior college education is equal for black and non-black
---------------------------------------------------------------------------------

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Create the jc times black interaction variable
data$jcXblack = data$jc*data$black
# Re-run the regression with the new interaction variable added
ols.lwage.jcblack.diff = lm(lwage~jc+univ+exper+black+jcXblack+hispanic+AA+BA+experXblack, data=data)
# Show the summary of the model
summary(ols.lwage.jcblack.diff)
```

Create the interaction variable jcXblack and add it to the model as follows:
$$lwage = \beta0 + \beta1jc+\beta2univ+\beta3exper+\beta4black+\beta5jcXblack+\beta6hispanic+\beta7AA+\beta8BA+\beta9experXblack+\epsilon$$

The coefficient for the jcXblack interaction variable ($\beta5$) now represents the difference in return to junior college between black and non-black, so we can set up our test as follows:\newline
Null Hypothesis: H0:  $\beta5=0$.\newline
Alternate Hypothesis: H1:  $\beta5\neq0$.\newline
Based on the p-value of 0.243 for $\beta5$, the test is not significant at the 0.05% significance level.  Therefore, we can't reject the null hypothesis that $\beta5=0$.  Said another way we cannot reject the null hypothesis that the return to junior college education is equal for black and non-black.\newline
\newline
Or alternatively, intuitively, we can see from the linear model, we derive:
$\frac{\delta log(wage)}{\delta jc} = \beta_{1}$\newline 
The model is specified in a way that the return to junior college education, 
is $\beta_{1}$, and is independent of ethnicity.
Therefore without additional computation, we can immediately answer that the return
on junior college education is the same for all ethnicities.

Question 5
==========

Test whether the return to university education is equal to the return to 1 year of working experience.
-----------------------------------------------------------------------------------
Original model:
$$lwage = \beta0 + \beta1jc+\beta2univ+\beta3exper+\beta4black+\beta5hispanic+\beta6AA+\beta7BA+\beta8experXblack+\epsilon$$

Convert the experience variable from months to years by creating a new variable experYr that divides the original variable exper by 12.  Replace the exper variable in the original model with this variable.
$$lwage = \beta0 + \beta1jc+\beta2univ+\beta3experYr+\beta4black+\beta5hispanic+\beta6AA+\beta7BA+\beta8experXblack+\epsilon$$

We would like to know if the $\beta2$ and $\beta3$ coefficients are the same or, equivalently, if their difference is 0.  We can define a variable $\theta$ such that $\theta = \beta2 - \beta3$ and rewrite our model like this:
$$lwage = \beta0 + \beta1jc+(\theta+\beta3)univ+\beta3experYr+\beta4black+\beta5hispanic+\beta6AA+\beta7BA+\beta8experXblack+\epsilon$$

Rewrite the model to get $\theta$ by itself as a coefficient:
$$lwage = \beta0 + \beta1jc+\theta{univ}+\beta3(univ+experYr)+\beta4black+\beta5hispanic+\beta6AA+\beta7BA+\beta8experXblack+\epsilon$$

Now our null hypothesis is $H0: \theta = 0$.\newline
Alternate Hypothesis: H1:  $\theta\neq0$.\newline

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Convert the exper variable from months to years by dividing it by 12.
data$experYr = data$exper/12
# Create a variable that is the sum of the univ and experYr variables
data$univ_plus_experYr = data$univ + data$experYr
# Rerun the regression with the new variables.
ols.lwage.univ.experYr = lm(lwage~jc+univ+univ_plus_experYr+black+hispanic+AA+BA+experXblack, data=data)
# Display a summary of the new model
summary(ols.lwage.univ.experYr)
```

Based on the very low p-value (0.000276) for $\theta$, the test is significant at the 0.05% significance level.  And even though the value of $\theta$ is close to 0 at 0.0129997, we can reject the null hypothesis that $\theta=0$.

Question 6
==========
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Show the summary of the model again
summary(ols.lwage.8ind)
```

Test the overall significance of this regression.
-------------------------------------------------
We are testing the overall significance of the original model as stated below:
$$lwage = \beta0 + \beta1jc+\beta2univ+\beta3exper+\beta4black+\beta5hispanic+\beta6AA+\beta7BA+\beta8experXblack+\epsilon$$

Here is the output from the summary of the original model.\newline
Residual standard error: 0.4287 on 6754 degrees of freedom\newline
Multiple R-squared:  0.2282,	Adjusted R-squared:  0.2272\newline
F-statistic: 249.6 on 8 and 6754 DF,  p-value: < 2.2e-16\newline

1. Our model null hypothesis is that there is no relationship among any of the independent variables and lwage variable. We are able to reject the null hypothesis since our p-value of the f-statistic of the model is significant at < 2.2e-16.
2. Practical significance: we have an R-squared value of 0.2282, indicating that 22.82% of the variation in lwage is explained by our model.\newline

Question 7
==========
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Define a square term for the exper variable
data$experXexper = data$exper*data$exper
# Add the new variable to the regression
ols.lwage.9ind = lm(lwage~jc+univ+exper+black+hispanic+AA+BA+experXblack+experXexper, data=data)
# Show the summary of the model
summary(ols.lwage.9ind)
```

Estimated return to work experience in this model
-------------------------------------------------
$$lwage = \beta0 + \beta1jc+\beta2univ+\beta3exper+\beta4black+\beta5hispanic+\beta6AA+\beta7BA+\beta8experXblack+\beta9experXexper$$

We obtain the return on a year of experience by evaluating:\newline
$$\frac{\delta lwage}{\delta exper} = \beta3 + \beta8black + 2*\beta9exper$$

$\beta3 = 4.301e-03= .004301$, $\beta8 = .001239$, $\beta9 = 000003379$\newline
Substituting these values in the the equation above we get:
$$\frac{\delta lwage}{\delta exper} = (.004301 - .001239*black + .000006758*exper)$$

Thus for blacks, the return to one year of working experience is:\newline
```{r}
.004301 - .001239 + .000006758
```
We interpret is as 3/10th of a percent of increase in wage per year of experience.

And for non-blacks, that return is:
```{r}
.004301 + .000006758
```
We interpret is as 4.3/10th of a percent of increase in wage per year of experience.

Question 8
==========
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=50)}
# Plot the graphs from the model
plot(ols.lwage.9ind)
# Show the summary of the model
summary(ols.lwage.9ind)
# Use the robust standard errors
coeftest(ols.lwage.9ind, vcov=vcovHC)
```

Homoskedasticity analysis:
--------------------------
We are testing homoskedasticity of the model with the quadratic experience term, expressed as:
$$lwage = \beta0 + \beta1jc+\beta2univ+\beta3exper+\beta4black+\beta5hispanic+\beta6AA+\beta7BA+\beta8experXblack+\beta9experXexper+\epsilon$$


We observe a small amount of heteroskedasticity from the plot:\newline
1 - We can see from the residuals vs fitted plot that the variance band changes very slightly as we move to higher fitted values.\newline
2 - The same story is told by the scale-location plot where we see that the smoothing line is not quite completely horizontal. Therefore we conclude a very small amount of heteroskedasticity and decide to use heteroskedasticity-robust methods for coefficient estimation.\newline
3 - We do not look at the Breusch Pagan test since we have a large number of observations, therefore we know almost certainly that we will obtain significance.

The implication of heteroskedastcity (even small) in the data is that the standard error of the univ coefficient ($\beta2$) may be biased. Biased standard errors can impact the outcomes of statistical tests.  Therefore, it can affect the testing of no effect of university education on salary, which is the t-test on the coefficient $\beta_{2}$

The $\beta2$ coefficient from the robust method was essentially unchanged, going from a value of $\beta2 = 7.382e-02$ using the non-robust estimation to $\beta2 = 7.3819e-02$ with robust estimation.
The standard error of the $\beta2$ coefficient was changed, going from 3.211e-03 using the non-robust estimation to 3.4501e-03 using the robust estimation.
However, even using the robust estimation, the p-value for our $\beta2$ coefficient remains significant at the 0.05, and we confirm that we must reject the null hypothesis that the coefficient value is null, with the meaning of that null hypothesis being that there is no relationship between time at university and wages.